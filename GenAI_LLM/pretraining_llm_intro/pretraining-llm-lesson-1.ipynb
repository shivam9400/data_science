{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af366c49-944d-4ad3-9bf9-cf0b5e386cc6",
   "metadata": {},
   "source": [
    "# Lesson 1: Why Pretraining?\n",
    "\n",
    "## Install dependencies and fix seed\n",
    "\n",
    "Welcome to Lesson 1!\n",
    "\n",
    "If you would like to access the `requirements.txt` file for this course, go to `File` and click on `Open`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb03d167-6ebc-4da9-87c8-11ce1bd4eeaa",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Install any packages if it does not exist\n",
    "# !pip install -q -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3d6b90b-7f1f-4f1f-a5fb-47bea82d3896",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T14:01:41.603088Z",
     "iopub.status.busy": "2025-06-06T14:01:41.602860Z",
     "iopub.status.idle": "2025-06-06T14:01:41.609176Z",
     "shell.execute_reply": "2025-06-06T14:01:41.608521Z",
     "shell.execute_reply.started": "2025-06-06T14:01:41.603064Z"
    },
    "height": 64,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Ignore insignificant warnings (ex: deprecations)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b23a1a8-096f-4563-ab87-bbf3c2e699a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T14:03:38.934051Z",
     "iopub.status.busy": "2025-06-06T14:03:38.933474Z",
     "iopub.status.idle": "2025-06-06T14:03:42.941429Z",
     "shell.execute_reply": "2025-06-06T14:03:42.940613Z",
     "shell.execute_reply.started": "2025-06-06T14:03:38.934027Z"
    },
    "height": 183,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set a seed for reproducibility\n",
    "import torch\n",
    "\n",
    "def fix_torch_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "fix_torch_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95414776-4bd9-4f87-93ce-2e4d1ec449b3",
   "metadata": {},
   "source": [
    "## Load a general pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f857edf6-c729-411f-943f-cbcced3c6470",
   "metadata": {},
   "source": [
    "**TinySolar-248m-4k** is a small **decoder-only** model with **248M parameters (similar in scale to GPT2) and a 4096 token** context window. We can find the model on the [Hugging Face model library](https://huggingface.co/upstage/TinySolar-248m-4k).\n",
    "\n",
    "We'll load the model in three steps:\n",
    "1. Specify the path to the model in the Hugging Face model library\n",
    "2. Load the model using `AutoModelforCausalLM` in the `transformers` library\n",
    "3. Load the tokenizer for the model from the same model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8405c72c-21d5-4a12-bbad-08a6e25be383",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T14:26:06.704764Z",
     "iopub.status.busy": "2025-06-06T14:26:06.704479Z",
     "iopub.status.idle": "2025-06-06T14:26:06.708483Z",
     "shell.execute_reply": "2025-06-06T14:26:06.707796Z",
     "shell.execute_reply.started": "2025-06-06T14:26:06.704744Z"
    },
    "height": 30,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "checkpoint = \"upstage/TinySolar-248m-4k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfe44f2-7376-4647-a0b3-719794316812",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T14:26:08.404459Z",
     "iopub.status.busy": "2025-06-06T14:26:08.404170Z",
     "iopub.status.idle": "2025-06-06T14:26:32.109073Z",
     "shell.execute_reply": "2025-06-06T14:26:32.108286Z",
     "shell.execute_reply.started": "2025-06-06T14:26:08.404439Z"
    },
    "height": 115,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "tiny_general_model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint,\n",
    "    device_map=\"auto\", # change to auto if you have access to a GPU, else cpu\n",
    "    torch_dtype=torch.bfloat16\n",
    "    #use_auth_token=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0156b2a-446d-40ec-9988-2b9542c7dfb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T14:54:12.337817Z",
     "iopub.status.busy": "2025-06-06T14:54:12.337529Z",
     "iopub.status.idle": "2025-06-06T14:54:12.342548Z",
     "shell.execute_reply": "2025-06-06T14:54:12.341967Z",
     "shell.execute_reply.started": "2025-06-06T14:54:12.337794Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(tiny_general_model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf36736-ba12-4c73-bedb-1aaab9007220",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T14:34:33.074053Z",
     "iopub.status.busy": "2025-06-06T14:34:33.073777Z",
     "iopub.status.idle": "2025-06-06T14:34:35.034030Z",
     "shell.execute_reply": "2025-06-06T14:34:35.033477Z",
     "shell.execute_reply.started": "2025-06-06T14:34:33.074035Z"
    },
    "height": 81,
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd3e68ab7de4ce790398690f7d39e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/966 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389b54cb2e5c4294bd09fb423712dd70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "997da936d1c14eca83656f307b040fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54018184f6cb432abf517838209a1b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tiny_general_tokenizer = AutoTokenizer.from_pretrained(checkpoint\n",
    "                                                      #use_auth_token=True\n",
    "                                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a15a3a-13a7-4ce5-a8fc-1635c4e2868c",
   "metadata": {},
   "source": [
    "## Generate text samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4d6924-b8a0-4e5b-a66d-e316e14a564d",
   "metadata": {},
   "source": [
    "Here we'll be generating some text with the model. We'll set a prompt, instantiate a text streamer, and then have the model complete the prompt.\n",
    "> A **text streamer** is a tool that allows to stream generated text token-by-token — rather than waiting for the entire output to be completed before displaying anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "804a5efc-45b1-4d93-8fde-df8a102d08e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T14:34:40.399313Z",
     "iopub.status.busy": "2025-06-06T14:34:40.398605Z",
     "iopub.status.idle": "2025-06-06T14:34:40.402449Z",
     "shell.execute_reply": "2025-06-06T14:34:40.401773Z",
     "shell.execute_reply.started": "2025-06-06T14:34:40.399284Z"
    },
    "height": 30,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt = \"I am an engineer. I love\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "400eeca5-67ea-437c-a8fe-98d4a66d90e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T14:56:34.040766Z",
     "iopub.status.busy": "2025-06-06T14:56:34.040185Z",
     "iopub.status.idle": "2025-06-06T14:56:34.044919Z",
     "shell.execute_reply": "2025-06-06T14:56:34.044221Z",
     "shell.execute_reply.started": "2025-06-06T14:56:34.040744Z"
    },
    "height": 30,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "inputs = tiny_general_tokenizer(prompt, \n",
    "                                return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b6600f4-e316-4229-adb3-38503b33f425",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T14:56:36.668613Z",
     "iopub.status.busy": "2025-06-06T14:56:36.668012Z",
     "iopub.status.idle": "2025-06-06T14:56:36.674563Z",
     "shell.execute_reply": "2025-06-06T14:56:36.673970Z",
     "shell.execute_reply.started": "2025-06-06T14:56:36.668588Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   315,   837,   396, 18112, 28723,   315,  2016]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "730710a2-eefb-4b35-a5c6-12f6c238c3f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T14:56:39.918631Z",
     "iopub.status.busy": "2025-06-06T14:56:39.918124Z",
     "iopub.status.idle": "2025-06-06T14:56:39.924368Z",
     "shell.execute_reply": "2025-06-06T14:56:39.923618Z",
     "shell.execute_reply.started": "2025-06-06T14:56:39.918594Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c1a9c24-5b1a-482c-a14a-90fc4334ceb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T14:42:49.697737Z",
     "iopub.status.busy": "2025-06-06T14:42:49.697203Z",
     "iopub.status.idle": "2025-06-06T14:42:49.703536Z",
     "shell.execute_reply": "2025-06-06T14:42:49.703013Z",
     "shell.execute_reply.started": "2025-06-06T14:42:49.697718Z"
    },
    "height": 115,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "streamer = TextStreamer(\n",
    "    tiny_general_tokenizer,\n",
    "    skip_prompt=True, # If you set to false, the model will first \n",
    "                      # return the prompt and then the generated text\n",
    "    skip_special_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6311307c-197c-4962-9b12-50b9076fb35a",
   "metadata": {},
   "source": [
    "Let's see few important keyword arguments of the **generate** method.\n",
    "* **use_cache**\n",
    "  * During generation, models generate tokens one at a time, using self-attention to look back at previous tokens.\n",
    "  * Without caching, the model recomputes the entire history every time it generates a new token.\n",
    "  * With caching, the model remembers intermediate results (key, value pairs) from previous steps and reuses them, which speeds things up.\n",
    "* **do_sample**\n",
    "  * When False, at each step, model picks the token with the highest probability.\n",
    "    * Output is deterministic — same input always gives same output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f41d5707-4ee1-4d82-a515-431235a3d775",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T14:57:19.205379Z",
     "iopub.status.busy": "2025-06-06T14:57:19.204722Z",
     "iopub.status.idle": "2025-06-06T14:57:22.089749Z",
     "shell.execute_reply": "2025-06-06T14:57:22.089248Z",
     "shell.execute_reply.started": "2025-06-06T14:57:19.205349Z"
    },
    "height": 166,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am an engineer. I love to travel and have been a part of the team since 2013.\n",
      "I'm a big fan of the music scene in London, so I was really excited when I saw this album on the radio. It's a great song about being a musician and having fun with it. The lyrics are very catchy and catchy, but they also make you feel like you're listening to something new.\n",
      "The album is called \"The Sound of Music\" and it's a great track that has a lot of different influences from other genres. It's got some cool songs such as \"Sweet\n"
     ]
    }
   ],
   "source": [
    "outputs = tiny_general_model.generate(\n",
    "    **inputs, \n",
    "    streamer=streamer, \n",
    "    use_cache=True,    # Reuses previous hidden states during generation\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False,   # Controls whether sampling is used to pick next tokens\n",
    "                       # False means greedy decoding is done - always pick the\n",
    "                       # highest probability token. True would allow randomness\n",
    "    temperature=0.0,   # this is valid only when do_sample = True\n",
    "    repetition_penalty=1.1  # Penalizes the model for repeating the same tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29640cf-3429-4e0c-be9c-a74833ddc29b",
   "metadata": {},
   "source": [
    "## Generate Python samples with pretrained general model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596ead58-c614-493e-9b35-40424fc06557",
   "metadata": {},
   "source": [
    "We will try to use the model to write a python function called `find_max()` that finds the maximum value in a list of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e87d6a02-ec21-4352-8dec-203bf2cacf16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T14:58:43.839771Z",
     "iopub.status.busy": "2025-06-06T14:58:43.839155Z",
     "iopub.status.idle": "2025-06-06T14:58:43.842907Z",
     "shell.execute_reply": "2025-06-06T14:58:43.842184Z",
     "shell.execute_reply.started": "2025-06-06T14:58:43.839749Z"
    },
    "height": 30,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt =  \"def find_max(numbers):\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "859db93d-c214-450b-8da3-57abfabf55b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T14:58:59.717939Z",
     "iopub.status.busy": "2025-06-06T14:58:59.717111Z",
     "iopub.status.idle": "2025-06-06T14:58:59.722472Z",
     "shell.execute_reply": "2025-06-06T14:58:59.721778Z",
     "shell.execute_reply.started": "2025-06-06T14:58:59.717913Z"
    },
    "height": 166,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "inputs = tiny_general_tokenizer(\n",
    "    prompt, return_tensors=\"pt\"\n",
    ").to(tiny_general_model.device)\n",
    "\n",
    "streamer = TextStreamer(\n",
    "    tiny_general_tokenizer, \n",
    "    skip_prompt=True, # Set to false to include the prompt in the output\n",
    "    skip_special_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de83f196-0118-4797-8894-c781a53c43a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T14:59:03.890156Z",
     "iopub.status.busy": "2025-06-06T14:59:03.889697Z",
     "iopub.status.idle": "2025-06-06T14:59:06.153745Z",
     "shell.execute_reply": "2025-06-06T14:59:06.153185Z",
     "shell.execute_reply.started": "2025-06-06T14:59:03.890133Z"
    },
    "height": 166,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       \"\"\"\n",
      "       Returns the number of times a user has been added to the list.\n",
      "       \"\"\"\n",
      "       return num_users() + 1\n",
      "\n",
      "   def get_user_id(self, id):\n",
      "       \"\"\"\n",
      "       Returns the number of users that have been added to the list.\n",
      "       \"\"\"\n",
      "       return self._get_user_id(id)\n",
      "\n",
      "   def get_user_name(self, name):\n",
      "       \"\"\"\n",
      "       Returns the name of the user that has been added to the list.\n",
      "       \"\"\"\n",
      "       return self._get_user_name(name\n"
     ]
    }
   ],
   "source": [
    "outputs = tiny_general_model.generate(\n",
    "    **inputs, \n",
    "    streamer=streamer, \n",
    "    use_cache=True, \n",
    "    max_new_tokens=128, \n",
    "    do_sample=False, \n",
    "    temperature=0.0, \n",
    "    repetition_penalty=1.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf5d693-9eb0-4394-8ad6-262ac5a48656",
   "metadata": {},
   "source": [
    "## Generate Python samples with finetuned Python model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075d7680-11de-4e4e-b507-9fae415baef1",
   "metadata": {},
   "source": [
    "This model has been fine-tuned on instruction code examples. \n",
    "\n",
    "More information about the fine-tuning datasets on the Hugging Face model library at [this link](https://huggingface.co/upstage/TinySolar-248m-4k-code-instruct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2c8669a4-384b-4de5-a4e3-7fecc81fd065",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T15:00:54.240610Z",
     "iopub.status.busy": "2025-06-06T15:00:54.239940Z",
     "iopub.status.idle": "2025-06-06T15:00:54.243821Z",
     "shell.execute_reply": "2025-06-06T15:00:54.243120Z",
     "shell.execute_reply.started": "2025-06-06T15:00:54.240585Z"
    },
    "height": 30,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "finetuned_checkpoint = \"upstage/TinySolar-248m-4k-code-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1aaf4752-c21d-44bd-8772-b1826d8f3bea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T15:00:55.361579Z",
     "iopub.status.busy": "2025-06-06T15:00:55.360978Z",
     "iopub.status.idle": "2025-06-06T15:00:58.830854Z",
     "shell.execute_reply": "2025-06-06T15:00:58.830169Z",
     "shell.execute_reply.started": "2025-06-06T15:00:55.361553Z"
    },
    "height": 166,
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3aad7d9ab904227904e5f10645b5d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/686 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519f4fb8da084e3ca7b4d5a8eee55a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09432df5a5884f109332668a801d1f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "610ec1c3fbfa4454a1529e9c05b3d1d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/966 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb1b658d35c44788dcc1c38e13e5a78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d2a16c656f24472857fb755362f8965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "495bab4830014a1a9683ef7955fa1f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tiny_finetuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    finetuned_checkpoint,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tiny_finetuned_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    finetuned_checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a9b9233-c384-45ee-9fbb-cf39611b83bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T15:40:18.176046Z",
     "iopub.status.busy": "2025-06-06T15:40:18.175561Z",
     "iopub.status.idle": "2025-06-06T15:40:20.184209Z",
     "shell.execute_reply": "2025-06-06T15:40:20.183514Z",
     "shell.execute_reply.started": "2025-06-06T15:40:18.176022Z"
    },
    "height": 370,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   if len(numbers) == 0:\n",
      "       return \"Invalid input\"\n",
      "   else:\n",
      "       return max(numbers)\n",
      "```\n",
      "\n",
      "In this solution, the `find_max` function takes a list of numbers as input and returns the maximum value in that list. It then iterates through each number in the list and checks if it is greater than or equal to 1. If it is, it adds it to the `max` list. Finally, it returns the maximum value found so far.\n"
     ]
    }
   ],
   "source": [
    "prompt =  \"def find_max(numbers):\"\n",
    "\n",
    "inputs = tiny_finetuned_tokenizer(\n",
    "    prompt, return_tensors=\"pt\"\n",
    ").to(tiny_finetuned_model.device)\n",
    "\n",
    "streamer = TextStreamer(\n",
    "    tiny_finetuned_tokenizer,\n",
    "    skip_prompt=True,\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "outputs = tiny_finetuned_model.generate(\n",
    "    **inputs,\n",
    "    streamer=streamer,\n",
    "    use_cache=True,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    "    repetition_penalty=1.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7331ab07-f883-4b93-ab66-6a6de1fc729d",
   "metadata": {},
   "source": [
    "## Generate Python samples with pretrained Python model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee8f283-71da-4736-9ac1-ba4ce9d5367f",
   "metadata": {},
   "source": [
    "Now, we will use a version of TinySolar-248m-4k that has been further pretrained (a process called **continued pretraining**) on a large selection of python code samples. \n",
    "\n",
    "The model can be found on Hugging Face at [this link](https://huggingface.co/upstage/TinySolar-248m-4k-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "30a3d3a3-1b57-4e56-98ad-58631485a58c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T15:41:59.600050Z",
     "iopub.status.busy": "2025-06-06T15:41:59.599765Z",
     "iopub.status.idle": "2025-06-06T15:41:59.603956Z",
     "shell.execute_reply": "2025-06-06T15:41:59.603137Z",
     "shell.execute_reply.started": "2025-06-06T15:41:59.600029Z"
    },
    "height": 30,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pretrained_checkpoint = \"upstage/TinySolar-248m-4k-py\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1df7a061-ca41-41c3-8ffa-533a3f557d16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T15:42:09.702914Z",
     "iopub.status.busy": "2025-06-06T15:42:09.702213Z",
     "iopub.status.idle": "2025-06-06T15:42:13.400079Z",
     "shell.execute_reply": "2025-06-06T15:42:13.399490Z",
     "shell.execute_reply.started": "2025-06-06T15:42:09.702891Z"
    },
    "height": 166,
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6ccae23f5284c5d9d7556f0717a09f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/639 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b07b659d574674878a8f8ad64c2bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d3e48e0c674be1a752b1b81f36e0d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79afe42444c4865ae2927e261c0ebdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/966 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21bee08ba5d1447e8e2e25b630aceb5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b24bfab6612410092c377174e65ac79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4fd6a4c0f2f4aad97d5a3ce8291ab33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tiny_custom_model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_checkpoint,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,    \n",
    ")\n",
    "\n",
    "tiny_custom_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "db2240fd-19d6-4f44-89dc-9325f4fdc6b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T15:42:15.815251Z",
     "iopub.status.busy": "2025-06-06T15:42:15.814964Z",
     "iopub.status.idle": "2025-06-06T15:42:17.912987Z",
     "shell.execute_reply": "2025-06-06T15:42:17.912282Z",
     "shell.execute_reply.started": "2025-06-06T15:42:15.815209Z"
    },
    "height": 336,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   \"\"\"Find the maximum number of numbers in a list.\"\"\"\n",
      "   max = 0\n",
      "   for num in numbers:\n",
      "       if num > max:\n",
      "           max = num\n",
      "   return max\n",
      "\n",
      "\n",
      "def get_min_max(numbers, min_value=1):\n",
      "   \"\"\"Get the minimum value of a list.\"\"\"\n",
      "   min_value = min_value or 1\n",
      "   for num in numbers:\n",
      "       if num < min_value:\n",
      "           min_value = num\n",
      "   return min_value\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"def find_max(numbers):\"\n",
    "\n",
    "inputs = tiny_custom_tokenizer(\n",
    "    prompt, return_tensors=\"pt\"\n",
    ").to(tiny_custom_model.device)\n",
    "\n",
    "streamer = TextStreamer(\n",
    "    tiny_custom_tokenizer,\n",
    "    skip_prompt=True, \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "outputs = tiny_custom_model.generate(\n",
    "    **inputs, streamer=streamer,\n",
    "    use_cache=True, \n",
    "    max_new_tokens=128, \n",
    "    do_sample=False, \n",
    "    repetition_penalty=1.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec94120",
   "metadata": {},
   "source": [
    "Try running the python code the model generated above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d109e788-2128-470d-8099-0a641938e062",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T15:42:20.553290Z",
     "iopub.status.busy": "2025-06-06T15:42:20.552993Z",
     "iopub.status.idle": "2025-06-06T15:42:20.557384Z",
     "shell.execute_reply": "2025-06-06T15:42:20.556657Z",
     "shell.execute_reply.started": "2025-06-06T15:42:20.553270Z"
    },
    "height": 115,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_max(numbers):\n",
    "   max = 0\n",
    "   for num in numbers:\n",
    "       if num > max:\n",
    "           max = num\n",
    "   return max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "868a767b-b5a1-4986-bef5-156a7e5a7acb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T15:42:22.267431Z",
     "iopub.status.busy": "2025-06-06T15:42:22.267142Z",
     "iopub.status.idle": "2025-06-06T15:42:22.271999Z",
     "shell.execute_reply": "2025-06-06T15:42:22.271462Z",
     "shell.execute_reply.started": "2025-06-06T15:42:22.267411Z"
    },
    "height": 30,
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_max([1,3,5,1,6,7,2])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
