{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae54c1f5-8f4f-4fc3-a970-9d3e65d23355",
   "metadata": {},
   "source": [
    "# Lesson 4: Preparing your model for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a12c7d5-f42f-4c78-b20a-b65c4ba6f9f0",
   "metadata": {},
   "source": [
    "*This notebook has lesson 4 and lesson 5.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca03d418-1a76-4399-bcdb-527de0978853",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:26:14.386412Z",
     "iopub.status.busy": "2025-06-07T11:26:14.386079Z",
     "iopub.status.idle": "2025-06-07T11:26:14.397217Z",
     "shell.execute_reply": "2025-06-07T11:26:14.396433Z",
     "shell.execute_reply.started": "2025-06-07T11:26:14.386357Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Ignore insignificant warnings (ex: deprecation warnings)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb5c998c-7135-4a0f-b3fa-9d2147f64eb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:26:14.398281Z",
     "iopub.status.busy": "2025-06-07T11:26:14.398061Z",
     "iopub.status.idle": "2025-06-07T11:26:16.227309Z",
     "shell.execute_reply": "2025-06-07T11:26:16.226718Z",
     "shell.execute_reply.started": "2025-06-07T11:26:14.398255Z"
    },
    "height": 251,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set a seed value for reproducibility\n",
    "import torch\n",
    "\n",
    "def fix_torch_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "fix_torch_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998a57a4-3af4-4fba-84b7-e4925a2bbe26",
   "metadata": {},
   "source": [
    "## Model configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582f9486-36ce-4361-9f0b-5bd2f190c332",
   "metadata": {},
   "source": [
    "Let's configure models based on Meta's Llama family of models. The transformers library has several tools for working with these models, refer [here](https://huggingface.co/docs/transformers/main/en/model_doc/llama).\n",
    "\n",
    "We will start by creating a `LlamaConfig` object to configure the architecture of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e85be0c-d2fa-4070-9450-fce9ec7b5312",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:26:16.228323Z",
     "iopub.status.busy": "2025-06-07T11:26:16.228002Z",
     "iopub.status.idle": "2025-06-07T11:26:17.001006Z",
     "shell.execute_reply": "2025-06-07T11:26:17.000157Z",
     "shell.execute_reply.started": "2025-06-07T11:26:16.228305Z"
    },
    "height": 64,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaConfig\n",
    "config = LlamaConfig()\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f440840",
   "metadata": {},
   "source": [
    "Next, update parameters to change the model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38993554-f050-479d-9f65-846f935af606",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:26:17.003506Z",
     "iopub.status.busy": "2025-06-07T11:26:17.002961Z",
     "iopub.status.idle": "2025-06-07T11:26:17.008444Z",
     "shell.execute_reply": "2025-06-07T11:26:17.007621Z",
     "shell.execute_reply.started": "2025-06-07T11:26:17.003485Z"
    },
    "height": 132,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config.num_hidden_layers = 12      # reduced from 32 to 12\n",
    "config.hidden_size = 1024          # reduced 1/4 from 4096 to 1024\n",
    "config.intermediate_size = 4096    # reduced 1/3 from 11008 to 4096 (dimension of MLP representations)\n",
    "config.num_key_value_heads = 8     # reduced 1/4 from 32 to 8 (defaults to num_attention_heads=32)\n",
    "config.torch_dtype = \"bfloat16\"    # for half-precision training\n",
    "config.use_cache = False           # `True` is incompatible w/ gradient checkpointing\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46001b8-f016-4002-aa61-34ae42f75df4",
   "metadata": {},
   "source": [
    "## Weight initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb71119-897d-4b0a-91fb-4d49decd5f50",
   "metadata": {},
   "source": [
    "In the next sections, we'll try four different ways to initialize the weights of a model for training:\n",
    "1. **Random weight initialization**\n",
    "2. **Using an existing model for continued pre-training**\n",
    "3. **Downscaling an existing model**\n",
    "4. **Upscaling an existing model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda505d9-0311-42e2-8f85-0220d98b67a5",
   "metadata": {},
   "source": [
    "### Random weight initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d47054-897d-4ab9-a382-6ea1aa824b88",
   "metadata": {},
   "source": [
    "Randomly initializing model weights sets all weights to values from a truncated normal distribution with mean 0 and standard deviation of 0.02. Values beyond 2-sigma from the mean are set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a860f758-3eb5-44bb-b920-f99d9c15e3bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:26:17.009575Z",
     "iopub.status.busy": "2025-06-07T11:26:17.009351Z",
     "iopub.status.idle": "2025-06-07T11:26:28.115478Z",
     "shell.execute_reply": "2025-06-07T11:26:28.113910Z",
     "shell.execute_reply.started": "2025-06-07T11:26:17.009549Z"
    },
    "height": 64,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 11:26:19.832338: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749295579.855825     272 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749295579.862930     272 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 1024)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "          (down_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((1024,), eps=1e-06)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((1024,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((1024,), eps=1e-06)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM\n",
    "model = LlamaForCausalLM(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b754ad49-4ab4-4768-8140-c00cc73a6933",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:26:28.117503Z",
     "iopub.status.busy": "2025-06-07T11:26:28.116324Z",
     "iopub.status.idle": "2025-06-07T11:26:28.122580Z",
     "shell.execute_reply": "2025-06-07T11:26:28.121673Z",
     "shell.execute_reply.started": "2025-06-07T11:26:28.117477Z"
    },
    "height": 115,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of parameters is: 342385664\n"
     ]
    }
   ],
   "source": [
    "def print_nparams(model):\n",
    "    \"\"\"Calculate the total number of model parameters\"\"\"\n",
    "    # For each parameter tensor p (like a matrix of weights), numel() \n",
    "    # returns the number of elements in that tensor\n",
    "    \n",
    "    nparams = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"The total number of parameters is: {nparams}\")\n",
    "\n",
    "print_nparams(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2350b11-b989-4b7b-8f60-0b53d6748b26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:26:28.123880Z",
     "iopub.status.busy": "2025-06-07T11:26:28.123578Z",
     "iopub.status.idle": "2025-06-07T11:26:28.142238Z",
     "shell.execute_reply": "2025-06-07T11:26:28.141622Z",
     "shell.execute_reply.started": "2025-06-07T11:26:28.123854Z"
    },
    "height": 132,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 30 weights of layer 'model.layers.0.self_attn.q_proj.weight':\n",
      "tensor([ 0.0217,  0.0204, -0.0008,  0.0087, -0.0089, -0.0291,  0.0166, -0.0086,\n",
      "         0.0004,  0.0017, -0.0089, -0.0095, -0.0135, -0.0160, -0.0148, -0.0131,\n",
      "         0.0104,  0.0200,  0.0348,  0.0110,  0.0082, -0.0011, -0.0233, -0.0113,\n",
      "         0.0087,  0.0267, -0.0030, -0.0272, -0.0098, -0.0089])\n"
     ]
    }
   ],
   "source": [
    "# Take a look at a sample of the weights in a single layer.\n",
    "layer_name = \"model.layers.0.self_attn.q_proj.weight\"\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if name == layer_name:\n",
    "        print(f\"First 30 weights of layer '{layer_name}':\")\n",
    "        print(param.data.view(-1)[:30])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb7f1e5b-ad7a-429d-9268-3dc1abadc881",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:26:28.143365Z",
     "iopub.status.busy": "2025-06-07T11:26:28.143057Z",
     "iopub.status.idle": "2025-06-07T11:26:38.353621Z",
     "shell.execute_reply": "2025-06-07T11:26:38.353029Z",
     "shell.execute_reply.started": "2025-06-07T11:26:28.143345Z"
    },
    "height": 455,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICK pom pom pom pom notable Heavy notable HeavyICKICKICKICK Heavy notable Heavy output notable Heavy output notable Heavy groundsICKICK groundsICK grounds Georg notable grounds grounds grounds grounds grounds grounds grounds grounds grounds很 similar很 similar很 similar grounds很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很很\n"
     ]
    }
   ],
   "source": [
    "# Load a tokenizer from Upstage Solar, \n",
    "# which is compatible with the Llama-2 tokenizer\n",
    "from transformers import LlamaTokenizer\n",
    "\n",
    "model_dir = \"upstage/SOLAR-10.7B-v1.0\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# Run simple inference with prompt\n",
    "from transformers import TextStreamer\n",
    "\n",
    "prompt = \"I am an engineer. I love\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "streamer = TextStreamer(\n",
    "    tokenizer, \n",
    "    skip_prompt=True, \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs, \n",
    "    streamer=streamer, \n",
    "    use_cache=True, \n",
    "    max_new_tokens=128, \n",
    "    do_sample=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2601dab8-8972-4398-892d-9dc1a4e25074",
   "metadata": {},
   "source": [
    "> With random initialized weights, the output does not make sense. This is because the model, uptil now, is not aware of any language and just throw out random tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dfc8e08-f68c-422f-96dd-d0515ab00f29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:26:38.354553Z",
     "iopub.status.busy": "2025-06-07T11:26:38.354333Z",
     "iopub.status.idle": "2025-06-07T11:26:38.819410Z",
     "shell.execute_reply": "2025-06-07T11:26:38.818627Z",
     "shell.execute_reply.started": "2025-06-07T11:26:38.354535Z"
    },
    "height": 115,
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the model from memory to avoid crashing the kernel\n",
    "# NOTE: We're running large models in a limited environment. \n",
    "import gc\n",
    "del model\n",
    "del streamer\n",
    "del outputs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d55a00-3402-48a7-8bce-81d8a546e213",
   "metadata": {},
   "source": [
    "### Reuse general pretrained model weights\n",
    "\n",
    "If you load an existing model, you can use it as is to continue pretraining on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b8a27a2-e56c-406f-bf2a-086f6c01a703",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:26:38.820355Z",
     "iopub.status.busy": "2025-06-07T11:26:38.820103Z",
     "iopub.status.idle": "2025-06-07T11:26:39.676420Z",
     "shell.execute_reply": "2025-06-07T11:26:39.675803Z",
     "shell.execute_reply.started": "2025-06-07T11:26:38.820338Z"
    },
    "height": 149,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_name_or_path = \"upstage/TinySolar-248m-4k\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dffa497e-c35f-459b-9000-0b8d4c601491",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:26:39.677392Z",
     "iopub.status.busy": "2025-06-07T11:26:39.677121Z",
     "iopub.status.idle": "2025-06-07T11:26:40.019635Z",
     "shell.execute_reply": "2025-06-07T11:26:40.018671Z",
     "shell.execute_reply.started": "2025-06-07T11:26:39.677374Z"
    },
    "height": 64,
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the model from memory to avoid crashing the kernel.\n",
    "# NOTE: We're running large models in a limited environment. \n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91cf37c-f8c6-45df-a7e2-95a8db9f1b93",
   "metadata": {},
   "source": [
    "### Downscaling from a general pretrained model\n",
    "\n",
    "Here we'll downscale the tinySolar-248m-4k model from a 12 layer model to a 10 layer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "404ac9d5-a5bd-42b4-bdad-c1954f98da87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:26:40.020730Z",
     "iopub.status.busy": "2025-06-07T11:26:40.020486Z",
     "iopub.status.idle": "2025-06-07T11:26:41.015922Z",
     "shell.execute_reply": "2025-06-07T11:26:41.015265Z",
     "shell.execute_reply.started": "2025-06-07T11:26:40.020712Z"
    },
    "height": 166,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig\n",
    "\n",
    "model_name_or_path = \"upstage/TinySolar-248m-4k\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e32988c2-07ad-4858-ad4d-336a1dda3be5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:26:41.019187Z",
     "iopub.status.busy": "2025-06-07T11:26:41.018971Z",
     "iopub.status.idle": "2025-06-07T11:26:41.024120Z",
     "shell.execute_reply": "2025-06-07T11:26:41.023211Z",
     "shell.execute_reply.started": "2025-06-07T11:26:41.019172Z"
    },
    "height": 30,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 1024)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (k_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "          (v_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "          (down_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((1024,), eps=1e-06)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((1024,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((1024,), eps=1e-06)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "813e119a-43cb-42c4-9a65-39adba7eb020",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:26:41.025563Z",
     "iopub.status.busy": "2025-06-07T11:26:41.025022Z",
     "iopub.status.idle": "2025-06-07T11:26:41.035554Z",
     "shell.execute_reply": "2025-06-07T11:26:41.034739Z",
     "shell.execute_reply.started": "2025-06-07T11:26:41.025537Z"
    },
    "height": 30,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of parameters is: 248013824\n"
     ]
    }
   ],
   "source": [
    "print_nparams(model)  # 248013824 => 248M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0784dfa5",
   "metadata": {},
   "source": [
    "We will now try to remove the middle two layers (layers 5 and 6) and update the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "130e3016-dd42-4a5d-aa2e-ba203c9bb745",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:26:41.036708Z",
     "iopub.status.busy": "2025-06-07T11:26:41.036415Z",
     "iopub.status.idle": "2025-06-07T11:26:41.261985Z",
     "shell.execute_reply": "2025-06-07T11:26:41.261222Z",
     "shell.execute_reply.started": "2025-06-07T11:26:41.036683Z"
    },
    "height": 183,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of parameters is: 217601024\n"
     ]
    }
   ],
   "source": [
    "layers = model.model.layers\n",
    "model.model.layers = layers[:5] + layers[-5:]\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name_or_path,    \n",
    "    num_hidden_layers=len(model.model.layers),\n",
    ")\n",
    "model.config = config\n",
    "\n",
    "print_nparams(model)  # 217601024 => 217M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3ea7cd",
   "metadata": {},
   "source": [
    "Clear the memory to avoid crashing the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35fa3dae-e6dd-4892-8965-b0a03d987456",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:26:41.263116Z",
     "iopub.status.busy": "2025-06-07T11:26:41.262826Z",
     "iopub.status.idle": "2025-06-07T11:26:41.604445Z",
     "shell.execute_reply": "2025-06-07T11:26:41.603748Z",
     "shell.execute_reply.started": "2025-06-07T11:26:41.263086Z"
    },
    "height": 81,
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: We're running large models in a limited environment.\n",
    "import gc\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54991d43-488d-4474-be1a-db0531961672",
   "metadata": {},
   "source": [
    "### Depth Upscaling from a general pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d39c6d3-50e0-4256-84c8-fe9981fba71b",
   "metadata": {},
   "source": [
    "Here we are going to upscale the tinySolar-248m-4k model from 12 layers to 16 layers. Here are steps to take,\n",
    "1. Configure a 16 layer model and initialize it with random weights\n",
    "2. Load the 12 layer tinySolar-248m-4k model into memory\n",
    "3. Copy the bottom 8 and top 8 layers from the 12 layer model and use them to overwrite the random weights of the 16 layer model\n",
    "4. Copy over the embedding and classifying layers to replace the randomly initialized counterparts in the 16 layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0472958a-7243-46f6-bd6c-6787286a819a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:26:41.606059Z",
     "iopub.status.busy": "2025-06-07T11:26:41.605215Z",
     "iopub.status.idle": "2025-06-07T11:26:41.616409Z",
     "shell.execute_reply": "2025-06-07T11:26:41.615644Z",
     "shell.execute_reply.started": "2025-06-07T11:26:41.606033Z"
    },
    "height": 183,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 32,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = LlamaConfig(\n",
    "    num_hidden_layers=16,  # We want our model to have 16 final layers\n",
    "    hidden_size=1024,\n",
    "    intermediate_size=4096,\n",
    "    num_attention_heads=32,\n",
    "    num_key_value_heads=8,\n",
    "    torch_dtype=\"bfloat16\",\n",
    "    use_cache=False \n",
    ")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8586992-1d23-45f7-86d9-480b34bc7894",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:26:41.617966Z",
     "iopub.status.busy": "2025-06-07T11:26:41.617316Z",
     "iopub.status.idle": "2025-06-07T11:26:46.545300Z",
     "shell.execute_reply": "2025-06-07T11:26:46.544396Z",
     "shell.execute_reply.started": "2025-06-07T11:26:41.617935Z"
    },
    "height": 64,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of parameters is: 308839424\n"
     ]
    }
   ],
   "source": [
    "model = LlamaForCausalLM(config)\n",
    "model = model.to(device=\"cuda\", dtype=torch.bfloat16) # convert to bfloat16\n",
    "print_nparams(model)  # 308839424 => 308M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "685b3dbe-3aa4-4858-b9d3-d59645108056",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:26:46.546781Z",
     "iopub.status.busy": "2025-06-07T11:26:46.546201Z",
     "iopub.status.idle": "2025-06-07T11:26:47.529614Z",
     "shell.execute_reply": "2025-06-07T11:26:47.528752Z",
     "shell.execute_reply.started": "2025-06-07T11:26:46.546753Z"
    },
    "height": 166,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of parameters is: 248013824\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = \"upstage/TinySolar-248m-4k\"\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,    \n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "print_nparams(pretrained_model) #  248013824 => 248M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acaaefbf-d73a-4ed0-bac6-b92575012952",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:26:47.530671Z",
     "iopub.status.busy": "2025-06-07T11:26:47.530453Z",
     "iopub.status.idle": "2025-06-07T11:26:47.567057Z",
     "shell.execute_reply": "2025-06-07T11:26:47.566159Z",
     "shell.execute_reply.started": "2025-06-07T11:26:47.530654Z"
    },
    "height": 183,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 32,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# [all layers except the last 4] + [all layers from index 4 onwards]\n",
    "# this result in duplicating the middle layers, effectively modifying the\n",
    "# architecture for experimentation.\n",
    "model.model.layers = deepcopy(pretrained_model.model.layers[:-4]) \\\n",
    "    + deepcopy(pretrained_model.model.layers[4:])\n",
    "model.model.layers = torch.nn.ModuleList([layer.to(model.device) for \n",
    "                                          layer in model.model.layers])\n",
    "\n",
    "# copy the embedding layer\n",
    "model.model.embed_tokens = deepcopy(pretrained_model.model.embed_tokens).to(model.device)\n",
    "\n",
    "# copy the final output projection layer\n",
    "model.lm_head = deepcopy(pretrained_model.lm_head).to(model.device)\n",
    "\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebdfdb4",
   "metadata": {},
   "source": [
    "Check the number of parameters is still 308 million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54dffe64-8451-4146-9c5d-cdfd43e43230",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:26:47.568108Z",
     "iopub.status.busy": "2025-06-07T11:26:47.567875Z",
     "iopub.status.idle": "2025-06-07T11:26:47.572823Z",
     "shell.execute_reply": "2025-06-07T11:26:47.571950Z",
     "shell.execute_reply.started": "2025-06-07T11:26:47.568090Z"
    },
    "height": 30,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of parameters is: 308839424\n"
     ]
    }
   ],
   "source": [
    "print_nparams(model)  # 308839424 => 308M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df68e10",
   "metadata": {},
   "source": [
    "Try using the model for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8a8a453-96cd-4f6c-91c5-5e529b2e41ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:26:47.573848Z",
     "iopub.status.busy": "2025-06-07T11:26:47.573582Z",
     "iopub.status.idle": "2025-06-07T11:26:50.922542Z",
     "shell.execute_reply": "2025-06-07T11:26:50.921973Z",
     "shell.execute_reply.started": "2025-06-07T11:26:47.573823Z"
    },
    "height": 319,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to work with people who are not afraid to look at the world and are not afraid to look at the world with a little bit of a twist.\n",
      "I am a very humble person and I am very fortunate to have a great team of people who work hard to make sure that I am a great role model for my family and friends.\n",
      "I am very fortunate to have a great team of people who are very passionate about their work and I am very fortunate to have a great team of people who are very passionate about their work and I am very fortunate to have a great team of people who are very passionate about their work and I\n"
     ]
    }
   ],
   "source": [
    "# Run simple inference to show no trained model\n",
    "prompt = \"I am an engineer. I love\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "streamer = TextStreamer(\n",
    "    tokenizer, \n",
    "    skip_prompt=True, \n",
    "    skip_special_tokens=True,\n",
    "    device = model.device\n",
    ")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     output_ids = model.generate(\n",
    "#         **inputs,\n",
    "#         max_new_tokens=128,\n",
    "#         do_sample=False\n",
    "#     )\n",
    "\n",
    "# output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "# print(output_text)\n",
    "outputs = model.generate(\n",
    "    **inputs, \n",
    "    streamer=streamer, \n",
    "    use_cache=True, \n",
    "    max_new_tokens=128, \n",
    "    do_sample=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1814e0d",
   "metadata": {},
   "source": [
    "### Save the model to disk\n",
    "\n",
    "Note the new model name here which reflects the 308 million parameters of the new, upscaled model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31215375-a0bd-4f9d-8ea2-1385c315c25b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:26:50.923464Z",
     "iopub.status.busy": "2025-06-07T11:26:50.923232Z",
     "iopub.status.idle": "2025-06-07T11:26:52.309316Z",
     "shell.execute_reply": "2025-06-07T11:26:52.308512Z",
     "shell.execute_reply.started": "2025-06-07T11:26:50.923447Z"
    },
    "height": 30,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_save_path = './data/TinySolar-308m-4k-init'\n",
    "model.save_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e38c2e-9dad-461e-90b7-211bc872844d",
   "metadata": {},
   "source": [
    "# Lesson 5. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52359c67-582a-4bbd-8a41-2360cfb33778",
   "metadata": {},
   "source": [
    "## Load the model to be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18424b16-3c97-4db8-9f3c-1ac6a421deb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:26:52.310894Z",
     "iopub.status.busy": "2025-06-07T11:26:52.310509Z",
     "iopub.status.idle": "2025-06-07T11:26:52.617359Z",
     "shell.execute_reply": "2025-06-07T11:26:52.616766Z",
     "shell.execute_reply.started": "2025-06-07T11:26:52.310862Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_save_path,    # where upstaged model was saved in previous lesson\n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_cache=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "498f6179-8fb6-4ecb-abb3-fc23748decb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:26:52.618352Z",
     "iopub.status.busy": "2025-06-07T11:26:52.618129Z",
     "iopub.status.idle": "2025-06-07T11:26:52.624445Z",
     "shell.execute_reply": "2025-06-07T11:26:52.623796Z",
     "shell.execute_reply.started": "2025-06-07T11:26:52.618336Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (down_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((1024,), eps=1e-06)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((1024,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((1024,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a30d4f-d6d6-46ad-a97d-4b86472099ff",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc61910-7c93-42bc-b139-eb0555cfab61",
   "metadata": {},
   "source": [
    "Here we'll update two methods on the `Dataset` object to allow it to interface with the trainer. These will be applied when we specify the dataset we created in Lesson 3 as the training data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92faf6a1-7de7-4f00-9906-e6028e445ce5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:26:52.625680Z",
     "iopub.status.busy": "2025-06-07T11:26:52.625410Z",
     "iopub.status.idle": "2025-06-07T11:26:52.848911Z",
     "shell.execute_reply": "2025-06-07T11:26:52.848290Z",
     "shell.execute_reply.started": "2025-06-07T11:26:52.625659Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This code defines a custom PyTorch dataset class that loads data \n",
    "from a Parquet file using the Hugging Face datasets library, and \n",
    "prepares it for training a language model.\n",
    "'''\n",
    "import datasets\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, args, split=\"train\"):\n",
    "        \"\"\"Initializes the custom dataset object.\"\"\"\n",
    "        self.args = args\n",
    "        self.dataset = datasets.load_dataset(\n",
    "            \"parquet\",\n",
    "            data_files=args.dataset_name,\n",
    "            split=split\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves a single data sample from the dataset \n",
    "        at the specified index\n",
    "        \"\"\"\n",
    "        # Convert the lists to a LongTensor for PyTorch\n",
    "        input_ids = torch.LongTensor(self.dataset[idx][\"input_ids\"])\n",
    "        labels = torch.LongTensor(self.dataset[idx][\"input_ids\"])\n",
    "\n",
    "        # Return the sample as a dictionary\n",
    "        return {\"input_ids\": input_ids, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4476dc1a-ba7b-414d-9ce6-fb0d21a1e8c2",
   "metadata": {},
   "source": [
    "## Configure Training Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2908b97e-18cc-40f6-b5a1-911f50af6377",
   "metadata": {},
   "source": [
    "Let's set up the training run. The training dataset we created in Lesson 3 is specified in the Dataset configuration section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6146d513-ba14-46d1-8a62-2b509697dba0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:26:52.850118Z",
     "iopub.status.busy": "2025-06-07T11:26:52.849603Z",
     "iopub.status.idle": "2025-06-07T11:26:52.876491Z",
     "shell.execute_reply": "2025-06-07T11:26:52.875937Z",
     "shell.execute_reply.started": "2025-06-07T11:26:52.850098Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import transformers\n",
    "\n",
    "@dataclass\n",
    "class CustomArguments(transformers.TrainingArguments):\n",
    "    dataset_name: str = field(                           # Dataset configuration\n",
    "        default=\"./working/packaged_pretrain_dataset.parquet\")\n",
    "    num_proc: int = field(default=1)                     # Number of subprocesses for data preprocessing\n",
    "    max_seq_length: int = field(default=32)              # Maximum sequence length\n",
    "\n",
    "    # Core training configurations\n",
    "    seed: int = field(default=0)                         # Random seed for initialization, ensuring reproducibility\n",
    "    optim: str = field(default=\"adamw_torch\")            # Optimizer, here it's AdamW implemented in PyTorch\n",
    "    max_steps: int = field(default=30)                   # Number of maximum training steps\n",
    "    per_device_train_batch_size: int = field(default=2)  # Batch size per device during training\n",
    "\n",
    "    # Other training configurations\n",
    "    learning_rate: float = field(default=5e-5)           # Initial learning rate for the optimizer\n",
    "    weight_decay: float = field(default=0)               # Weight decay\n",
    "    warmup_steps: int = field(default=10)                # Number of steps for the learning rate warmup phase\n",
    "    lr_scheduler_type: str = field(default=\"linear\")     # Type of learning rate scheduler\n",
    "    gradient_checkpointing: bool = field(default=True)   # Enable gradient checkpointing to save memory\n",
    "    dataloader_num_workers: int = field(default=2)       # Number of subprocesses for data loading\n",
    "    bf16: bool = field(default=True)                     # Use bfloat16 precision for training on supported hardware\n",
    "    gradient_accumulation_steps: int = field(default=1)  # Number of steps to accumulate gradients before updating model weights\n",
    "    \n",
    "    # Logging configuration\n",
    "    logging_steps: int = field(default=3)                # Frequency of logging training information\n",
    "    report_to: str = field(default=\"none\")               # Destination for logging (e.g., WandB, TensorBoard)\n",
    "\n",
    "    # Saving configuration\n",
    "    # save_strategy: str = field(default=\"steps\")          # Can be replaced with \"epoch\"\n",
    "    # save_steps: int = field(default=3)                   # Frequency of saving training checkpoint\n",
    "    # save_total_limit: int = field(default=2)             # The total number of checkpoints to be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7533561-6ce8-402e-b5a7-8056c62e78e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:26:52.877368Z",
     "iopub.status.busy": "2025-06-07T11:26:52.877148Z",
     "iopub.status.idle": "2025-06-07T11:26:52.911806Z",
     "shell.execute_reply": "2025-06-07T11:26:52.911222Z",
     "shell.execute_reply.started": "2025-06-07T11:26:52.877351Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create a parser that knows how to parse fields defined in CustomArguments \n",
    "# dataclass (which includes TrainingArguments fields + your custom ones).\n",
    "parser = transformers.HfArgumentParser(CustomArguments)\n",
    "args, = parser.parse_args_into_dataclasses(\n",
    "    args=[\"--output_dir\", \"output\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1f2b93c-6c1f-4e16-aae1-d9b3d063015d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:26:52.912931Z",
     "iopub.status.busy": "2025-06-07T11:26:52.912637Z",
     "iopub.status.idle": "2025-06-07T11:26:53.628062Z",
     "shell.execute_reply": "2025-06-07T11:26:53.627210Z",
     "shell.execute_reply.started": "2025-06-07T11:26:52.912893Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d55cbcc7-9665-4e4a-a297-8466cbb5d894",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:26:53.629141Z",
     "iopub.status.busy": "2025-06-07T11:26:53.628934Z",
     "iopub.status.idle": "2025-06-07T11:26:53.634012Z",
     "shell.execute_reply": "2025-06-07T11:26:53.633179Z",
     "shell.execute_reply.started": "2025-06-07T11:26:53.629125Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "print(\"Input shape: \", train_dataset[0]['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b3e38f-49af-40f9-93b2-d6b6df3f942a",
   "metadata": {},
   "source": [
    "## Run the trainer and monitor the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24c905c0-7b38-4bb9-b925-2b60a8d8e151",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:26:53.635154Z",
     "iopub.status.busy": "2025-06-07T11:26:53.634846Z",
     "iopub.status.idle": "2025-06-07T11:26:53.927784Z",
     "shell.execute_reply": "2025-06-07T11:26:53.927150Z",
     "shell.execute_reply.started": "2025-06-07T11:26:53.635136Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "set up a callback to log the loss values during training\n",
    "'''\n",
    "from transformers import Trainer, TrainingArguments, TrainerCallback\n",
    "\n",
    "# Define a custom callback to log the loss values\n",
    "class LossLoggingCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            self.logs.append(logs)\n",
    "\n",
    "    def __init__(self):\n",
    "        self.logs = []\n",
    "\n",
    "# Initialize the callback\n",
    "loss_logging_callback = LossLoggingCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "209be4b1-e26d-4807-a685-d54b5f2bad92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:26:53.928800Z",
     "iopub.status.busy": "2025-06-07T11:26:53.928520Z",
     "iopub.status.idle": "2025-06-07T11:27:04.515783Z",
     "shell.execute_reply": "2025-06-07T11:27:04.515036Z",
     "shell.execute_reply.started": "2025-06-07T11:26:53.928776Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:09, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.519700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.468100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4.413700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>4.804900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>4.517900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>4.849500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.763700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>4.778400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>4.008900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>4.191900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=30, training_loss=4.431664848327637, metrics={'train_runtime': 10.0723, 'train_samples_per_second': 5.957, 'train_steps_per_second': 2.978, 'total_flos': 3180342804480.0, 'train_loss': 4.431664848327637, 'epoch': 0.0003754646374888925})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "create an instance of the Hugging Face Trainer object from the \n",
    "transformers library. \n",
    "Call the train() method of the trainder to initialize the training run.\n",
    "'''\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=pretrained_model, \n",
    "    args=args, \n",
    "    train_dataset=train_dataset, \n",
    "    eval_dataset=None,\n",
    "    callbacks=[loss_logging_callback] \n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de34ed7-fedd-4072-ab0b-bf769af19073",
   "metadata": {},
   "source": [
    "## Checking the performance of intermediate checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91b59554-287f-4ee3-a86a-a0482fedace6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:27:04.516939Z",
     "iopub.status.busy": "2025-06-07T11:27:04.516680Z",
     "iopub.status.idle": "2025-06-07T11:27:04.835029Z",
     "shell.execute_reply": "2025-06-07T11:27:04.834179Z",
     "shell.execute_reply.started": "2025-06-07T11:27:04.516912Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TextStreamer\n",
    "model_name_or_path = \"upstage/TinySolar-248m-4k\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e73bfdda-18f0-4ca3-a929-7800589b306d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:27:04.836447Z",
     "iopub.status.busy": "2025-06-07T11:27:04.835911Z",
     "iopub.status.idle": "2025-06-07T11:27:09.227176Z",
     "shell.execute_reply": "2025-06-07T11:27:09.226389Z",
     "shell.execute_reply.started": "2025-06-07T11:27:04.836426Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TextStreamer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name_or_path = \"/kaggle/working/output/checkpoint-30\"\n",
    "model2 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a3005bb-5098-4ab3-a276-5f0b6b94fbfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:27:09.228224Z",
     "iopub.status.busy": "2025-06-07T11:27:09.228013Z",
     "iopub.status.idle": "2025-06-07T11:27:10.808757Z",
     "shell.execute_reply": "2025-06-07T11:27:10.808159Z",
     "shell.execute_reply.started": "2025-06-07T11:27:09.228207Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the people who go there, so it is because for me, my passion to become the next leader of your life, is that it is this way. So I have a friend and I've got five years of coaching. I grew my business during that time and, I've had that for four years.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I am an engineer. I love\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model2.device)\n",
    "\n",
    "streamer = TextStreamer(\n",
    "    tokenizer, \n",
    "    skip_prompt=True, \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "outputs = model2.generate(\n",
    "    **inputs, \n",
    "    streamer=streamer, \n",
    "    use_cache=True, \n",
    "    max_new_tokens=64,     \n",
    "    do_sample=True,\n",
    "    temperature=1.0,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7609634,
     "sourceId": 12088265,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
