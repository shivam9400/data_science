{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section talks about how pipeline class in transformer module works. A general workflow looks like - a pipeline object is initiated from transformer library, to which raw text is fed directly as input and labels are given as output.\n",
    "\n",
    "Pipeline takes care of multiple things internally,\n",
    "1. Tokenization\n",
    "2. Feeding to model and generating logits\n",
    "3. Post processing logits and giving output as labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go through further text in this notebook, it is important to understand difference between **AutoModelForSequenceClassification, AutoModel, and BertModel**.\n",
    "\n",
    "| Method | What does it do? | Use case |\n",
    "| --- | --- | --- |\n",
    "| **AutoModel** | Loads base model like BERT, **without and task head** | when we want to extract embeddings |\n",
    "| **BertModel** | Same as AutoModel, but specifc to BERT. This again, does not have any task specific head. | Useful when we want to work directly with BERTâ€™s embeddings or use it as the base for other models |\n",
    "| **AutoModelForSequenceClassification** | Loads a base model with a classification head on top. It automatically adds a classification head on top of the pre-trained model. | Directly use or fine-tune the model for classification tasks. |\n",
    "\n",
    "Other AutoModelFor... are,\n",
    "\n",
    "[\"AutoModelForSequenceClassification\", \"AutoModelForTokenClassification\",\"AutoModelForQuestionAnswering\", \"AutoModelForMaskedLM\", \"AutoModelForCausalLM\", \"AutoModelForSeq2SeqLM\", \"AutoModelForMultipleChoice\", \"AutoModelForNextSentencePrediction\", \"AutoModelForImageClassification\", \"AutoModelForVision2Seq\", \"AutoModelForSpeechSeq2Seq\", \"AutoModelForAudioClassification\", \"AutoModelForCTC\", \"AutoModelForImageToText\", \"AutoModelForZeroShotObjectDetection\", \"AutoModelForDepthEstimation\", \"AutoModelForDocumentQuestionAnswering\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:29.165073Z",
     "iopub.status.busy": "2025-06-06T10:26:29.164862Z",
     "iopub.status.idle": "2025-06-06T10:26:30.861685Z",
     "shell.execute_reply": "2025-06-06T10:26:30.860847Z",
     "shell.execute_reply.started": "2025-06-06T10:26:29.165056Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "# checking if gpu device is available\n",
    "import torch\n",
    "print(torch.cuda.is_available(), torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:30.862748Z",
     "iopub.status.busy": "2025-06-06T10:26:30.862438Z",
     "iopub.status.idle": "2025-06-06T10:26:38.068174Z",
     "shell.execute_reply": "2025-06-06T10:26:38.067605Z",
     "shell.execute_reply.started": "2025-06-06T10:26:30.862728Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 10:26:32.866434: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749205592.914727     150 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749205592.928396     150 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiating classifier (using Huggingface's pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- initiating instance of class **pipeline** and calling \"sentiment-analysis\" pre-trained model.\n",
    "- When we call pipeline, There are following things running in the background,\n",
    "  -  Tokenizer --> it takes in the raw input (text) and tokenizes it and convert into IDs.\n",
    "  -  Model --> The IDs fed to model gives logits.\n",
    "  -  Post processing --> These logits are then processed to give predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:38.069472Z",
     "iopub.status.busy": "2025-06-06T10:26:38.068886Z",
     "iopub.status.idle": "2025-06-06T10:26:38.763129Z",
     "shell.execute_reply": "2025-06-06T10:26:38.762550Z",
     "shell.execute_reply.started": "2025-06-06T10:26:38.069451Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:38.763989Z",
     "iopub.status.busy": "2025-06-06T10:26:38.763787Z",
     "iopub.status.idle": "2025-06-06T10:26:38.959006Z",
     "shell.execute_reply": "2025-06-06T10:26:38.958180Z",
     "shell.execute_reply.started": "2025-06-06T10:26:38.763973Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598047137260437},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The output tells that first sentence's sentiment is positive with 0.959 probability while second sentence's sentiment is negative with .999 probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer AutoModel and AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A checkpoint in transformers is a saved state of the model during or after training. It includes the model weights, allowing to resume training or use the model later without retraining. \n",
    "> Checkpoints are useful for fault recovery, model evaluation, and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:38.960171Z",
     "iopub.status.busy": "2025-06-06T10:26:38.959876Z",
     "iopub.status.idle": "2025-06-06T10:26:38.963810Z",
     "shell.execute_reply": "2025-06-06T10:26:38.963053Z",
     "shell.execute_reply.started": "2025-06-06T10:26:38.960147Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting tokenizer and pre-trained model (from a checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:38.966212Z",
     "iopub.status.busy": "2025-06-06T10:26:38.965996Z",
     "iopub.status.idle": "2025-06-06T10:26:40.494986Z",
     "shell.execute_reply": "2025-06-06T10:26:40.494188Z",
     "shell.execute_reply.started": "2025-06-06T10:26:38.966195Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing raw inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is unlike \"pipeline\". Here, tokenization is done manually using the AutoTokenizer called on a checkpoint, and feeding raw input to this.\n",
    "\n",
    "Output is a dictionary with following keys,\n",
    "1. input_ids\n",
    "2. attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:40.495984Z",
     "iopub.status.busy": "2025-06-06T10:26:40.495794Z",
     "iopub.status.idle": "2025-06-06T10:26:40.502991Z",
     "shell.execute_reply": "2025-06-06T10:26:40.502262Z",
     "shell.execute_reply.started": "2025-06-06T10:26:40.495969Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, \n",
    "                   return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:40.504146Z",
     "iopub.status.busy": "2025-06-06T10:26:40.503880Z",
     "iopub.status.idle": "2025-06-06T10:26:40.517652Z",
     "shell.execute_reply": "2025-06-06T10:26:40.517000Z",
     "shell.execute_reply.started": "2025-06-06T10:26:40.504128Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding tokenized inputs to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model(**inputs) unpacks the inputs dictionary to have something like this,\n",
    "> **model(input_ids=...  , attention_mask=...)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:40.518685Z",
     "iopub.status.busy": "2025-06-06T10:26:40.518258Z",
     "iopub.status.idle": "2025-06-06T10:26:40.594369Z",
     "shell.execute_reply": "2025-06-06T10:26:40.593598Z",
     "shell.execute_reply.started": "2025-06-06T10:26:40.518668Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[-0.1798,  0.2333,  0.6321,  ..., -0.3017,  0.5008,  0.1481],\n",
       "         [ 0.2758,  0.6497,  0.3200,  ..., -0.0760,  0.5136,  0.1329],\n",
       "         [ 0.9046,  0.0985,  0.2950,  ...,  0.3352, -0.1407, -0.6464],\n",
       "         ...,\n",
       "         [ 0.1466,  0.5661,  0.3235,  ..., -0.3376,  0.5100, -0.0561],\n",
       "         [ 0.7500,  0.0487,  0.1738,  ...,  0.4684,  0.0030, -0.6084],\n",
       "         [ 0.0519,  0.3729,  0.5223,  ...,  0.3584,  0.6500, -0.3883]],\n",
       "\n",
       "        [[-0.2937,  0.7283, -0.1497,  ..., -0.1187, -1.0227, -0.0422],\n",
       "         [-0.2206,  0.9384, -0.0951,  ..., -0.3643, -0.6605,  0.2407],\n",
       "         [-0.1536,  0.8988, -0.0728,  ..., -0.2189, -0.8528,  0.0710],\n",
       "         ...,\n",
       "         [-0.3017,  0.9002, -0.0200,  ..., -0.1082, -0.8412, -0.0861],\n",
       "         [-0.3338,  0.9674, -0.0729,  ..., -0.1952, -0.8181, -0.0634],\n",
       "         [-0.3454,  0.8824, -0.0426,  ..., -0.0993, -0.8329, -0.1065]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector output by the Transformer module is usually large. It generally has three dimensions:\n",
    "* Batch size: The number of sequences processed at a time (2 in above example).\n",
    "* Sequence length: The length of the numerical representation of the sequence (16 in above example).\n",
    "* Hidden size: The vector dimension of each model input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:40.595451Z",
     "iopub.status.busy": "2025-06-06T10:26:40.595189Z",
     "iopub.status.idle": "2025-06-06T10:26:40.600149Z",
     "shell.execute_reply": "2025-06-06T10:26:40.599377Z",
     "shell.execute_reply.started": "2025-06-06T10:26:40.595424Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Let's see shape of last hidden state\n",
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The **model heads** take the high-dimensional vector of hidden states as input and project them onto a different dimension. They are usually composed of one or a few linear layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different architectures available in Transformers library (with attached heads), with each one designed around tackling a specific task. We will explore the sequence classification architecture. Let's see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using architecture with model heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some key points to consider,\n",
    "1. We are using the same checkpoint that was defined above.\n",
    "2. We are feeding the tokenized inputs to the model (created from tokenzier from same checkpoint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:40.601169Z",
     "iopub.status.busy": "2025-06-06T10:26:40.600901Z",
     "iopub.status.idle": "2025-06-06T10:26:40.786042Z",
     "shell.execute_reply": "2025-06-06T10:26:40.785253Z",
     "shell.execute_reply.started": "2025-06-06T10:26:40.601146Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:40.787206Z",
     "iopub.status.busy": "2025-06-06T10:26:40.786975Z",
     "iopub.status.idle": "2025-06-06T10:26:40.792993Z",
     "shell.execute_reply": "2025-06-06T10:26:40.792332Z",
     "shell.execute_reply.started": "2025-06-06T10:26:40.787190Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.5607,  1.6123],\n",
       "         [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>),\n",
       " torch.Size([2, 2]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits, outputs.logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> the model head takes as input the high-dimensional vectors, and outputs vectors containing two values, one for each label.\n",
    "\n",
    "> Since we have two sentences and two labels, the result we get from our model is of shape 2 x 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post processing the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The output are not probabilities but logits, the raw, unnormalized scores outputted by the last layer of the model. \n",
    "* To be converted to probabilities, they need to go through a SoftMax layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:40.793760Z",
     "iopub.status.busy": "2025-06-06T10:26:40.793581Z",
     "iopub.status.idle": "2025-06-06T10:26:40.805846Z",
     "shell.execute_reply": "2025-06-06T10:26:40.805223Z",
     "shell.execute_reply.started": "2025-06-06T10:26:40.793745Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0195e-02, 9.5980e-01],\n",
      "        [9.9946e-01, 5.4419e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:40.806669Z",
     "iopub.status.busy": "2025-06-06T10:26:40.806472Z",
     "iopub.status.idle": "2025-06-06T10:26:40.819984Z",
     "shell.execute_reply": "2025-06-06T10:26:40.819390Z",
     "shell.execute_reply.started": "2025-06-06T10:26:40.806654Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means,\n",
    "1. 1st sentence is positive.\n",
    "2. 2nd sentence is negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Till now, we have seen following usage of the **transformer** library,\n",
    "\n",
    "<img src = transformer_lib.jpg width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Pre-trained BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing BERT with random parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:40.821083Z",
     "iopub.status.busy": "2025-06-06T10:26:40.820835Z",
     "iopub.status.idle": "2025-06-06T10:26:42.459598Z",
     "shell.execute_reply": "2025-06-06T10:26:42.459013Z",
     "shell.execute_reply.started": "2025-06-06T10:26:40.821058Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Building the config\n",
    "config = BertConfig()  # The configuration contains many attributes \n",
    "                       # that are used to build the model.\n",
    "\n",
    "# Building the model from the config \n",
    "\n",
    "model = BertModel(config) # this initializes the model randomly [not pre-trained]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:42.460445Z",
     "iopub.status.busy": "2025-06-06T10:26:42.460226Z",
     "iopub.status.idle": "2025-06-06T10:26:42.465821Z",
     "shell.execute_reply": "2025-06-06T10:26:42.465246Z",
     "shell.execute_reply.started": "2025-06-06T10:26:42.460430Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.51.3\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instead, Loading pre-trained BERT ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:42.466799Z",
     "iopub.status.busy": "2025-06-06T10:26:42.466521Z",
     "iopub.status.idle": "2025-06-06T10:26:43.373465Z",
     "shell.execute_reply": "2025-06-06T10:26:43.372671Z",
     "shell.execute_reply.started": "2025-06-06T10:26:42.466781Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:43.374527Z",
     "iopub.status.busy": "2025-06-06T10:26:43.374244Z",
     "iopub.status.idle": "2025-06-06T10:26:43.380104Z",
     "shell.execute_reply": "2025-06-06T10:26:43.379485Z",
     "shell.execute_reply.started": "2025-06-06T10:26:43.374504Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:43.381039Z",
     "iopub.status.busy": "2025-06-06T10:26:43.380833Z",
     "iopub.status.idle": "2025-06-06T10:26:44.309545Z",
     "shell.execute_reply": "2025-06-06T10:26:44.308728Z",
     "shell.execute_reply.started": "2025-06-06T10:26:43.381024Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"sample_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:44.310801Z",
     "iopub.status.busy": "2025-06-06T10:26:44.310457Z",
     "iopub.status.idle": "2025-06-06T10:26:44.317206Z",
     "shell.execute_reply": "2025-06-06T10:26:44.316283Z",
     "shell.execute_reply.started": "2025-06-06T10:26:44.310772Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]\n",
    "encoded_sequences = [\n",
    "    [101, 7592, 999, 102],\n",
    "    [101, 4658, 1012, 102],\n",
    "    [101, 3835, 999, 102],\n",
    "]\n",
    "\n",
    "model_inputs = torch.tensor(encoded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:44.325385Z",
     "iopub.status.busy": "2025-06-06T10:26:44.324978Z",
     "iopub.status.idle": "2025-06-06T10:26:44.337627Z",
     "shell.execute_reply": "2025-06-06T10:26:44.336800Z",
     "shell.execute_reply.started": "2025-06-06T10:26:44.325363Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 7592,  999,  102],\n",
       "        [ 101, 4658, 1012,  102],\n",
       "        [ 101, 3835,  999,  102]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:44.338572Z",
     "iopub.status.busy": "2025-06-06T10:26:44.338347Z",
     "iopub.status.idle": "2025-06-06T10:26:44.420370Z",
     "shell.execute_reply": "2025-06-06T10:26:44.419545Z",
     "shell.execute_reply.started": "2025-06-06T10:26:44.338556Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output = model(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:44.421719Z",
     "iopub.status.busy": "2025-06-06T10:26:44.421213Z",
     "iopub.status.idle": "2025-06-06T10:26:44.426086Z",
     "shell.execute_reply": "2025-06-06T10:26:44.425374Z",
     "shell.execute_reply.started": "2025-06-06T10:26:44.421683Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 768])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.last_hidden_state.shape  # [batch size, features, hidden states]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling multiple sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:44.427045Z",
     "iopub.status.busy": "2025-06-06T10:26:44.426841Z",
     "iopub.status.idle": "2025-06-06T10:26:44.705699Z",
     "shell.execute_reply": "2025-06-06T10:26:44.705177Z",
     "shell.execute_reply.started": "2025-06-06T10:26:44.427030Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Defining a checkpoint and loading a tokenzier and model from that checkpoint\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "# Defining an input sequence\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "# creating tokens from the tokenizer, converting tokens to ids and then tensor\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = torch.tensor([ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:44.706839Z",
     "iopub.status.busy": "2025-06-06T10:26:44.706582Z",
     "iopub.status.idle": "2025-06-06T10:26:44.711868Z",
     "shell.execute_reply": "2025-06-06T10:26:44.711240Z",
     "shell.execute_reply.started": "2025-06-06T10:26:44.706815Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the tokenizer has divided the input sentence to 14 token IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:44.712645Z",
     "iopub.status.busy": "2025-06-06T10:26:44.712478Z",
     "iopub.status.idle": "2025-06-06T10:26:44.770788Z",
     "shell.execute_reply": "2025-06-06T10:26:44.769902Z",
     "shell.execute_reply.started": "2025-06-06T10:26:44.712632Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How about multiple inputs now ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:44.771831Z",
     "iopub.status.busy": "2025-06-06T10:26:44.771610Z",
     "iopub.status.idle": "2025-06-06T10:26:44.776676Z",
     "shell.execute_reply": "2025-06-06T10:26:44.776059Z",
     "shell.execute_reply.started": "2025-06-06T10:26:44.771815Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we are going to use the pad token id from the tokenizer object. \n",
    "# let's see what it is\n",
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:44.778166Z",
     "iopub.status.busy": "2025-06-06T10:26:44.777431Z",
     "iopub.status.idle": "2025-06-06T10:26:44.974964Z",
     "shell.execute_reply": "2025-06-06T10:26:44.974364Z",
     "shell.execute_reply.started": "2025-06-06T10:26:44.778142Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence1_ids = [[200, 200, 200]] # these are IDs from tokenizer for sentence 1\n",
    "sequence2_ids = [[200, 200]]      # these are IDs from tokenzier for sentence 2\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "print(model(torch.tensor(batched_ids)).logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Interesting to see that results are not matching for sequence 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring in the attention masks to have the same results ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:44.976274Z",
     "iopub.status.busy": "2025-06-06T10:26:44.975824Z",
     "iopub.status.idle": "2025-06-06T10:26:45.014152Z",
     "shell.execute_reply": "2025-06-06T10:26:45.013396Z",
     "shell.execute_reply.started": "2025-06-06T10:26:44.976247Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "attention_mask = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "outputs = model(torch.tensor(batched_ids), \n",
    "                attention_mask=torch.tensor(attention_mask))\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning a pretrained model (General Flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:45.015215Z",
     "iopub.status.busy": "2025-06-06T10:26:45.015003Z",
     "iopub.status.idle": "2025-06-06T10:26:45.328707Z",
     "shell.execute_reply": "2025-06-06T10:26:45.327904Z",
     "shell.execute_reply.started": "2025-06-06T10:26:45.015198Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Import needed libraries\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Defining a checkpoint and then initializing a tokenizer and model object\n",
    "# using that checkpoint\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "# Define raw text as input\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important keyword argument for tokenizer object,\n",
    "* **padding=True**\n",
    "  * Pads all sequences in the batch to the length of the longest one. Ensures uniform input size (required for batching).\n",
    "* **truncation=True**\n",
    "  * Truncates sequences longer than the modelâ€™s max length (e.g., 512 for BERT). Prevents overflow and out-of-memory errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:45.330274Z",
     "iopub.status.busy": "2025-06-06T10:26:45.329584Z",
     "iopub.status.idle": "2025-06-06T10:26:45.336610Z",
     "shell.execute_reply": "2025-06-06T10:26:45.335881Z",
     "shell.execute_reply.started": "2025-06-06T10:26:45.330254Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
       "          2607,  2026,  2878,  2166,  1012,   102],\n",
       "        [  101,  2023,  2607,  2003,  6429,   999,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating batch from the raw input text and tokenizer object\n",
    "batch = tokenizer(sequences, \n",
    "                  padding=True, \n",
    "                  truncation=True, \n",
    "                  return_tensors=\"pt\")\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:45.337454Z",
     "iopub.status.busy": "2025-06-06T10:26:45.337237Z",
     "iopub.status.idle": "2025-06-06T10:26:45.350104Z",
     "shell.execute_reply": "2025-06-06T10:26:45.349512Z",
     "shell.execute_reply.started": "2025-06-06T10:26:45.337439Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:45.350973Z",
     "iopub.status.busy": "2025-06-06T10:26:45.350766Z",
     "iopub.status.idle": "2025-06-06T10:26:45.362175Z",
     "shell.execute_reply": "2025-06-06T10:26:45.361431Z",
     "shell.execute_reply.started": "2025-06-06T10:26:45.350951Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Since, we are creating training example, we will also need labels.\n",
    "# for this case, lets give a label of 1 to both the input training example.\n",
    "batch[\"labels\"] = torch.tensor([1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:45.363287Z",
     "iopub.status.busy": "2025-06-06T10:26:45.363032Z",
     "iopub.status.idle": "2025-06-06T10:26:47.175352Z",
     "shell.execute_reply": "2025-06-06T10:26:47.174786Z",
     "shell.execute_reply.started": "2025-06-06T10:26:45.363265Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# create a optimizer to update model parameters\n",
    "optimizer = AdamW(model.parameters(), \n",
    "                  lr= 5e-2)\n",
    "\n",
    "# Runs the model on the input batch, and computes the loss.\n",
    "# batch is expected to be a dictionary with keys like,\n",
    "# input_ids, attention_mask, and possibly labels.\n",
    "loss = model(**batch).loss\n",
    "\n",
    "# Computes the gradients of the loss with respect to the model parameters\n",
    "loss.backward()\n",
    "\n",
    "# Applies the calculated gradients to update the model weights\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning a pretrained model (using Trainer Class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the MRPC (Microsoft Research Paraphrase Corpus) dataset, introduced in a paper by William B. Dolan and Chris Brockett. The dataset consists of 5,801 pairs of sentences, with a label indicating if they are paraphrases or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:47.176737Z",
     "iopub.status.busy": "2025-06-06T10:26:47.176450Z",
     "iopub.status.idle": "2025-06-06T10:26:50.534219Z",
     "shell.execute_reply": "2025-06-06T10:26:50.533585Z",
     "shell.execute_reply.started": "2025-06-06T10:26:47.176711Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:50.535483Z",
     "iopub.status.busy": "2025-06-06T10:26:50.534920Z",
     "iopub.status.idle": "2025-06-06T10:26:50.540583Z",
     "shell.execute_reply": "2025-06-06T10:26:50.539942Z",
     "shell.execute_reply.started": "2025-06-06T10:26:50.535461Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:50.541422Z",
     "iopub.status.busy": "2025-06-06T10:26:50.541180Z",
     "iopub.status.idle": "2025-06-06T10:26:50.554893Z",
     "shell.execute_reply": "2025-06-06T10:26:50.554169Z",
     "shell.execute_reply.started": "2025-06-06T10:26:50.541405Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:50.555791Z",
     "iopub.status.busy": "2025-06-06T10:26:50.555600Z",
     "iopub.status.idle": "2025-06-06T10:26:50.700383Z",
     "shell.execute_reply": "2025-06-06T10:26:50.699704Z",
     "shell.execute_reply.started": "2025-06-06T10:26:50.555772Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:50.701629Z",
     "iopub.status.busy": "2025-06-06T10:26:50.701379Z",
     "iopub.status.idle": "2025-06-06T10:26:50.705850Z",
     "shell.execute_reply": "2025-06-06T10:26:50.705100Z",
     "shell.execute_reply.started": "2025-06-06T10:26:50.701609Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], \n",
    "                     example[\"sentence2\"], \n",
    "                     truncation=True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **batched=True**\n",
    "\n",
    "> Instead of applying the function to one example at a time, it passes a batch (a list of examples) to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:50.706984Z",
     "iopub.status.busy": "2025-06-06T10:26:50.706718Z",
     "iopub.status.idle": "2025-06-06T10:26:51.108918Z",
     "shell.execute_reply": "2025-06-06T10:26:51.108375Z",
     "shell.execute_reply.started": "2025-06-06T10:26:50.706954Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc833568b2c449e98418993d4db78d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:51.109813Z",
     "iopub.status.busy": "2025-06-06T10:26:51.109608Z",
     "iopub.status.idle": "2025-06-06T10:26:51.114813Z",
     "shell.execute_reply": "2025-06-06T10:26:51.114105Z",
     "shell.execute_reply.started": "2025-06-06T10:26:51.109797Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 3668\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:51.115739Z",
     "iopub.status.busy": "2025-06-06T10:26:51.115567Z",
     "iopub.status.idle": "2025-06-06T10:26:51.130594Z",
     "shell.execute_reply": "2025-06-06T10:26:51.130038Z",
     "shell.execute_reply.started": "2025-06-06T10:26:51.115726Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "    num_rows: 3668\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**\n",
    "* Dynamic padding is a strategy where sequences in a batch are padded only up to the length of the longest sequence in that batch, instead of a fixed maximum length.\n",
    "* With dynamic padding, different batches canâ€”and usually doâ€”have different sequence lengths, because each batch is padded only up to its longest sequence.\n",
    "  * Models expect input tensors to be uniform in size within a batch.\n",
    "  * But between batches, lengths can vary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> pad all the examples to the length of the longest element when we batch elements together\n",
    "\n",
    "> The function that is responsible for putting together samples inside a batch is called a **collate function**.\n",
    "\n",
    "\n",
    "**DataCollatorWithPadding** - It takes a tokenizer when you instantiate it,\n",
    "  * to know which padding token to use\n",
    "  * and whether the model expects padding to be on the left or on the right of the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:51.131529Z",
     "iopub.status.busy": "2025-06-06T10:26:51.131301Z",
     "iopub.status.idle": "2025-06-06T10:26:51.144218Z",
     "shell.execute_reply": "2025-06-06T10:26:51.143655Z",
     "shell.execute_reply.started": "2025-06-06T10:26:51.131514Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# DataCollatorWithPadding - Automatically pads each batch \n",
    "#                           dynamically to the length of the longest \n",
    "#                           sequence in that batch\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:51.145147Z",
     "iopub.status.busy": "2025-06-06T10:26:51.144887Z",
     "iopub.status.idle": "2025-06-06T10:26:51.158890Z",
     "shell.execute_reply": "2025-06-06T10:26:51.158376Z",
     "shell.execute_reply.started": "2025-06-06T10:26:51.145123Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['label', 'input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see how it is working - FIRST REMOVE UNWANTED COLUMNS FROM THE DATASET\n",
    "samples = {k: v for k, v in tokenized_datasets[\"train\"][:8].items() \n",
    "           if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
    "samples.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:51.159759Z",
     "iopub.status.busy": "2025-06-06T10:26:51.159564Z",
     "iopub.status.idle": "2025-06-06T10:26:51.172864Z",
     "shell.execute_reply": "2025-06-06T10:26:51.172299Z",
     "shell.execute_reply.started": "2025-06-06T10:26:51.159743Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 59, 47, 67, 59, 50, 62, 32]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check lengths of tokens in each training example provided\n",
    "[len(x) for x in samples[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:51.173840Z",
     "iopub.status.busy": "2025-06-06T10:26:51.173580Z",
     "iopub.status.idle": "2025-06-06T10:26:51.187348Z",
     "shell.execute_reply": "2025-06-06T10:26:51.186793Z",
     "shell.execute_reply.started": "2025-06-06T10:26:51.173816Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 67]),\n",
       " 'token_type_ids': torch.Size([8, 67]),\n",
       " 'attention_mask': torch.Size([8, 67]),\n",
       " 'labels': torch.Size([8])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here's how data collator will work\n",
    "batch = data_collator(samples)\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning with trainer API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Transformers provides a Trainer class to help fine-tune any of the pretrained models.\n",
    "* The first step before defining **Trainer** is to **define a TrainingArguments class** that will contain all the hyperparameters the Trainer will use for training and evaluation.\n",
    "  * The <u>only argument to be provided is a directory where the trained model will be saved</u>, as well as the checkpoints along the way.\n",
    "  * Rest can be left as defaults, which will work pretty well for a basic fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:51.188129Z",
     "iopub.status.busy": "2025-06-06T10:26:51.187951Z",
     "iopub.status.idle": "2025-06-06T10:26:51.224456Z",
     "shell.execute_reply": "2025-06-06T10:26:51.223703Z",
     "shell.execute_reply.started": "2025-06-06T10:26:51.188115Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\",\n",
    "                                 report_to=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:51.225528Z",
     "iopub.status.busy": "2025-06-06T10:26:51.225295Z",
     "iopub.status.idle": "2025-06-06T10:26:51.406353Z",
     "shell.execute_reply": "2025-06-06T10:26:51.405575Z",
     "shell.execute_reply.started": "2025-06-06T10:26:51.225512Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, \n",
    "                                                           num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:51.407265Z",
     "iopub.status.busy": "2025-06-06T10:26:51.407068Z",
     "iopub.status.idle": "2025-06-06T10:26:51.650945Z",
     "shell.execute_reply": "2025-06-06T10:26:51.650369Z",
     "shell.execute_reply.started": "2025-06-06T10:26:51.407248Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:26:51.652011Z",
     "iopub.status.busy": "2025-06-06T10:26:51.651740Z",
     "iopub.status.idle": "2025-06-06T10:29:02.834411Z",
     "shell.execute_reply": "2025-06-06T10:29:02.833454Z",
     "shell.execute_reply.started": "2025-06-06T10:26:51.651986Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 02:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.492200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.237400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.2942901561462801, metrics={'train_runtime': 130.751, 'train_samples_per_second': 84.16, 'train_steps_per_second': 10.531, 'total_flos': 405114969714960.0, 'train_loss': 0.2942901561462801, 'epoch': 3.0})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, fine tuning\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:29:02.835868Z",
     "iopub.status.busy": "2025-06-06T10:29:02.835537Z",
     "iopub.status.idle": "2025-06-06T10:29:06.103067Z",
     "shell.execute_reply": "2025-06-06T10:29:06.101953Z",
     "shell.execute_reply.started": "2025-06-06T10:29:02.835840Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.31.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.18)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:29:06.104466Z",
     "iopub.status.busy": "2025-06-06T10:29:06.104198Z",
     "iopub.status.idle": "2025-06-06T10:29:07.234869Z",
     "shell.execute_reply": "2025-06-06T10:29:07.234060Z",
     "shell.execute_reply.started": "2025-06-06T10:29:06.104437Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(408, 2) (408,)\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:29:07.235900Z",
     "iopub.status.busy": "2025-06-06T10:29:07.235654Z",
     "iopub.status.idle": "2025-06-06T10:29:08.657268Z",
     "shell.execute_reply": "2025-06-06T10:29:08.656516Z",
     "shell.execute_reply.started": "2025-06-06T10:29:07.235879Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.162256 ,  4.407572 ],\n",
       "       [ 2.5285764, -4.0733857]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.predictions[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:29:08.658448Z",
     "iopub.status.busy": "2025-06-06T10:29:08.658154Z",
     "iopub.status.idle": "2025-06-06T10:29:10.510818Z",
     "shell.execute_reply": "2025-06-06T10:29:10.510007Z",
     "shell.execute_reply.started": "2025-06-06T10:29:08.658421Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1 1\n",
      " 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 0 0 0 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 0\n",
      " 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1\n",
      " 1]\n",
      "metrics: EvaluationModule(name: \"glue\", module_type: \"metric\", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: \"\"\"\n",
      "Compute GLUE evaluation metric associated to each GLUE dataset.\n",
      "Args:\n",
      "    predictions: list of predictions to score.\n",
      "        Each translation should be tokenized into a list of tokens.\n",
      "    references: list of lists of references for each translation.\n",
      "        Each reference should be tokenized into a list of tokens.\n",
      "Returns: depending on the GLUE subset, one or several of:\n",
      "    \"accuracy\": Accuracy\n",
      "    \"f1\": F1 score\n",
      "    \"pearson\": Pearson Correlation\n",
      "    \"spearmanr\": Spearman Correlation\n",
      "    \"matthews_correlation\": Matthew Correlation\n",
      "Examples:\n",
      "\n",
      "    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of [\"mnli\", \"mnli_mismatched\", \"mnli_matched\", \"qnli\", \"rte\", \"wnli\", \"hans\"]\n",
      "    >>> references = [0, 1]\n",
      "    >>> predictions = [0, 1]\n",
      "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
      "    >>> print(results)\n",
      "    {'accuracy': 1.0}\n",
      "\n",
      "    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'\n",
      "    >>> references = [0, 1]\n",
      "    >>> predictions = [0, 1]\n",
      "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
      "    >>> print(results)\n",
      "    {'accuracy': 1.0, 'f1': 1.0}\n",
      "\n",
      "    >>> glue_metric = evaluate.load('glue', 'stsb')\n",
      "    >>> references = [0., 1., 2., 3., 4., 5.]\n",
      "    >>> predictions = [0., 1., 2., 3., 4., 5.]\n",
      "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
      "    >>> print({\"pearson\": round(results[\"pearson\"], 2), \"spearmanr\": round(results[\"spearmanr\"], 2)})\n",
      "    {'pearson': 1.0, 'spearmanr': 1.0}\n",
      "\n",
      "    >>> glue_metric = evaluate.load('glue', 'cola')\n",
      "    >>> references = [0, 1]\n",
      "    >>> predictions = [0, 1]\n",
      "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
      "    >>> print(results)\n",
      "    {'matthews_correlation': 1.0}\n",
      "\"\"\", stored examples: 0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "preds = np.argmax(predictions.predictions, axis=-1) # this line tells which index\n",
    "                                                    # has max value in the array\n",
    "print(\"preds: {}\".format(preds))\n",
    "\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)\n",
    "print(\"metrics: {}\".format(metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:29:10.511962Z",
     "iopub.status.busy": "2025-06-06T10:29:10.511661Z",
     "iopub.status.idle": "2025-06-06T10:29:10.516360Z",
     "shell.execute_reply": "2025-06-06T10:29:10.515546Z",
     "shell.execute_reply.started": "2025-06-06T10:29:10.511928Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-defining the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:29:10.517456Z",
     "iopub.status.busy": "2025-06-06T10:29:10.517201Z",
     "iopub.status.idle": "2025-06-06T10:29:10.989588Z",
     "shell.execute_reply": "2025-06-06T10:29:10.988885Z",
     "shell.execute_reply.started": "2025-06-06T10:29:10.517429Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\"test-trainer\", \n",
    "                                  eval_strategy=\"epoch\",\n",
    "                                  fp16=True,\n",
    "                                  no_cuda=False,\n",
    "                                  num_train_epochs=3,\n",
    "                                  report_to=\"none\"  # no logging to wandb\n",
    "                                 )\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, \n",
    "                                                           num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:29:10.990893Z",
     "iopub.status.busy": "2025-06-06T10:29:10.990705Z",
     "iopub.status.idle": "2025-06-06T10:31:24.219703Z",
     "shell.execute_reply": "2025-06-06T10:31:24.219117Z",
     "shell.execute_reply.started": "2025-06-06T10:29:10.990879Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 02:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.417061</td>\n",
       "      <td>0.828431</td>\n",
       "      <td>0.883721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.552400</td>\n",
       "      <td>0.428832</td>\n",
       "      <td>0.845588</td>\n",
       "      <td>0.890052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.332400</td>\n",
       "      <td>0.679138</td>\n",
       "      <td>0.848039</td>\n",
       "      <td>0.895623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.3718539413197281, metrics={'train_runtime': 132.6482, 'train_samples_per_second': 82.956, 'train_steps_per_second': 10.381, 'total_flos': 405114969714960.0, 'train_loss': 0.3718539413197281, 'epoch': 3.0})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning a pretrained model (without Trainer Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:31:24.220708Z",
     "iopub.status.busy": "2025-06-06T10:31:24.220487Z",
     "iopub.status.idle": "2025-06-06T10:31:27.675708Z",
     "shell.execute_reply": "2025-06-06T10:31:27.675148Z",
     "shell.execute_reply.started": "2025-06-06T10:31:24.220690Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06181d23e0fe4fbc857bdcf4f5b7b098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f0c7d2770e541c2a02eab5bcc0a6d0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "207128dfb04c4a9696a265f3faca10e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "# get dataset\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "\n",
    "# define a checkpoint from which model and tokenizer will be loaded\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "\n",
    "# get the tokenizer from the checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], \n",
    "                     example[\"sentence2\"], \n",
    "                     truncation=True)\n",
    "\n",
    "# batched tokenization of the raw dataset\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "# data collator for preparing batches to feed to model\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to apply a bit of postprocessing to tokenized_datasets, to take care of some things that the **Trainer** automatically did.\n",
    "* Remove the columns corresponding to values the model does not expect (like the sentence1 and sentence2 columns).\n",
    "* Rename the column label to labels.\n",
    "* Set the format of the datasets so they return PyTorch tensors instead of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:31:27.676696Z",
     "iopub.status.busy": "2025-06-06T10:31:27.676429Z",
     "iopub.status.idle": "2025-06-06T10:31:27.689888Z",
     "shell.execute_reply": "2025-06-06T10:31:27.689355Z",
     "shell.execute_reply.started": "2025-06-06T10:31:27.676671Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labels', 'input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Things done by trainer\n",
    "# 1. remove unwanted columns\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \n",
    "                                                        \"sentence2\", \n",
    "                                                        \"idx\"])\n",
    "# 2. ensure predicted column name is \"labels\"\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# 3. setting dataset format to torch\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "tokenized_datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:31:27.690830Z",
     "iopub.status.busy": "2025-06-06T10:31:27.690546Z",
     "iopub.status.idle": "2025-06-06T10:31:29.277615Z",
     "shell.execute_reply": "2025-06-06T10:31:29.276809Z",
     "shell.execute_reply.started": "2025-06-06T10:31:27.690807Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Things done by trainer\n",
    "# 4. DataLoader - groups your dataset into mini-batches efficiently\n",
    "#               - Shuffle the data\n",
    "#               - collate / pad examples\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], \n",
    "    shuffle=True, \n",
    "    batch_size=8, \n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], \n",
    "    batch_size=8, \n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:31:29.278533Z",
     "iopub.status.busy": "2025-06-06T10:31:29.278302Z",
     "iopub.status.idle": "2025-06-06T10:31:29.301216Z",
     "shell.execute_reply": "2025-06-06T10:31:29.300356Z",
     "shell.execute_reply.started": "2025-06-06T10:31:29.278517Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': torch.Size([8]),\n",
       " 'input_ids': torch.Size([8, 81]),\n",
       " 'token_type_ids': torch.Size([8, 81]),\n",
       " 'attention_mask': torch.Size([8, 81])}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:31:29.302117Z",
     "iopub.status.busy": "2025-06-06T10:31:29.301920Z",
     "iopub.status.idle": "2025-06-06T10:31:30.092599Z",
     "shell.execute_reply": "2025-06-06T10:31:30.091904Z",
     "shell.execute_reply.started": "2025-06-06T10:31:29.302102Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "459\n"
     ]
    }
   ],
   "source": [
    "batch_count=0\n",
    "for batch in train_dataloader:\n",
    "    batch_count+=1\n",
    "\n",
    "print(batch_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding to Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:31:30.093554Z",
     "iopub.status.busy": "2025-06-06T10:31:30.093312Z",
     "iopub.status.idle": "2025-06-06T10:31:30.620553Z",
     "shell.execute_reply": "2025-06-06T10:31:30.619722Z",
     "shell.execute_reply.started": "2025-06-06T10:31:30.093536Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7701, grad_fn=<NllLossBackward0>) torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, \n",
    "                                                           num_labels=2)\n",
    "outputs = model(**batch)\n",
    "print(outputs.loss, outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:31:30.621644Z",
     "iopub.status.busy": "2025-06-06T10:31:30.621359Z",
     "iopub.status.idle": "2025-06-06T10:31:30.645613Z",
     "shell.execute_reply": "2025-06-06T10:31:30.644904Z",
     "shell.execute_reply.started": "2025-06-06T10:31:30.621615Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:31:30.646774Z",
     "iopub.status.busy": "2025-06-06T10:31:30.646577Z",
     "iopub.status.idle": "2025-06-06T10:31:30.651668Z",
     "shell.execute_reply": "2025-06-06T10:31:30.650982Z",
     "shell.execute_reply.started": "2025-06-06T10:31:30.646757Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1377\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:31:30.657832Z",
     "iopub.status.busy": "2025-06-06T10:31:30.657644Z",
     "iopub.status.idle": "2025-06-06T10:31:30.781494Z",
     "shell.execute_reply": "2025-06-06T10:31:30.780865Z",
     "shell.execute_reply.started": "2025-06-06T10:31:30.657817Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:31:30.782420Z",
     "iopub.status.busy": "2025-06-06T10:31:30.782183Z",
     "iopub.status.idle": "2025-06-06T10:33:23.856580Z",
     "shell.execute_reply": "2025-06-06T10:33:23.855826Z",
     "shell.execute_reply.started": "2025-06-06T10:31:30.782403Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ac060353ff40b782a9749700cd491e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()    # puts the model in training mode --> enables batchnorm, dropout\n",
    "for epoch in range(num_epochs):  # iterate through all epochs\n",
    "    for batch in train_dataloader:  # iterate through mini-batches of data\n",
    "        batch = {k: v.to(device) for k, v in batch.items()} # move batch to device\n",
    "        outputs = model(**batch)  # FORWARD PASS\n",
    "        loss = outputs.loss       # compute loss\n",
    "        loss.backward()           # BACKWARD PASS\n",
    "\n",
    "        optimizer.step()          # update model weights\n",
    "        lr_scheduler.step()       # adjusts the learning rate\n",
    "        optimizer.zero_grad()     # resets gradient before next batch\n",
    "        progress_bar.update(1)    # advances progress bar by 1 after each batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:33:23.858110Z",
     "iopub.status.busy": "2025-06-06T10:33:23.857461Z",
     "iopub.status.idle": "2025-06-06T10:33:25.502848Z",
     "shell.execute_reply": "2025-06-06T10:33:25.502219Z",
     "shell.execute_reply.started": "2025-06-06T10:33:23.858090Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8725490196078431, 'f1': 0.9103448275862069}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")   # loading metrics\n",
    "\n",
    "model.eval()    # setting the model in evaluation mode now...\n",
    "for batch in eval_dataloader:  # iterating through batches\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}   # sending data to device\n",
    "    with torch.no_grad():    # this ensures that no grads are calculated\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits    # get logits\n",
    "    predictions = torch.argmax(logits, dim=-1)   # get predictions\n",
    "    \n",
    "    # feeds this batch's preds and true labels to metric tracker\n",
    "    # this will accumulate across batches\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"]) \n",
    "\n",
    "# After all batches are processed, this computes the final evaluation results.\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supercharge training loop with \"Accelerate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:33:25.503815Z",
     "iopub.status.busy": "2025-06-06T10:33:25.503558Z",
     "iopub.status.idle": "2025-06-06T10:33:25.507513Z",
     "shell.execute_reply": "2025-06-06T10:33:25.506852Z",
     "shell.execute_reply.started": "2025-06-06T10:33:25.503785Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from accelerate import Accelerator     # addition\n",
    "# from torch.optim import AdamW\n",
    "# from transformers import AutoModelForSequenceClassification, get_scheduler\n",
    "\n",
    "# accelerator = Accelerator()            # addition\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "# optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "# # addition\n",
    "# train_dl, eval_dl, model, optimizer = accelerator.prepare(\n",
    "#     train_dataloader, eval_dataloader, model, optimizer\n",
    "# )\n",
    "\n",
    "# num_epochs = 3\n",
    "# num_training_steps = num_epochs * len(train_dl)\n",
    "# lr_scheduler = get_scheduler(\n",
    "#     \"linear\",\n",
    "#     optimizer=optimizer,\n",
    "#     num_warmup_steps=0,\n",
    "#     num_training_steps=num_training_steps,\n",
    "# )\n",
    "\n",
    "# progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "# model.train()\n",
    "# for epoch in range(num_epochs):\n",
    "#     for batch in train_dl:\n",
    "#         outputs = model(**batch)\n",
    "#         loss = outputs.loss\n",
    "#         accelerator.backward(loss)     # addition\n",
    "\n",
    "#         optimizer.step()\n",
    "#         lr_scheduler.step()\n",
    "#         optimizer.zero_grad()\n",
    "#         progress_bar.update(1)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
