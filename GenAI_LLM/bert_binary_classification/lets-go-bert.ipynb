{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11861972,"sourceType":"datasetVersion","datasetId":7453862}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Basic Intro","metadata":{}},{"cell_type":"markdown","source":"* BERT - Bidirectional Encoder Representations from Transformer\n* It was developed by Google AI in 2018 and jointly conditions on both left and right context. This mean unlike older models that read text left-to-right or right-to-left, **BERT reads in both directions simultaneously**.\n* As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create models for a wide range of NLP tasks.","metadata":{}},{"cell_type":"markdown","source":"## Why BERT?","metadata":{}},{"cell_type":"markdown","source":"* Context-free models such as **word2vec** or **GloVe** generate a **single word embedding representation** for each word in the vocabulary.\n* BERT, as a contextual model, can capture different meanings of a word in a bidirectional way.","metadata":{}},{"cell_type":"markdown","source":"**Working of BERT**,\n\n| Step | Description | Example |\n| :- | -: | :-: |\n| 1 | Input Text | my dog likes playing.\n| 2 | Tokenization - [WordPiece](https://www.youtube.com/watch?v=qpv6ms_t_1A&t=11s) | [CLS], my, dog, likes, play, ##ing, [SEP]\n| 3 | Vectorization | Token embeddings + Segment embeddings + Position embeddings\n| 4 | Pre-training Task 1 | Masked Language Modelling; [MASK]\n| 5 | Pre-training Task 2 | Next sentence prediction; IsNext v/s NotNext","metadata":{}},{"cell_type":"markdown","source":"## BERT Implementation","metadata":{}},{"cell_type":"markdown","source":"### Spam v/s Ham Detection","metadata":{}},{"cell_type":"markdown","source":"#### Import Libraries and Dataset","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport transformers\nfrom transformers import AutoModel, BertTokenizerFast","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:38:56.424779Z","iopub.execute_input":"2025-05-19T09:38:56.425492Z","iopub.status.idle":"2025-05-19T09:38:56.429217Z","shell.execute_reply.started":"2025-05-19T09:38:56.425465Z","shell.execute_reply":"2025-05-19T09:38:56.428485Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# specify GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:38:56.433165Z","iopub.execute_input":"2025-05-19T09:38:56.433387Z","iopub.status.idle":"2025-05-19T09:38:56.448543Z","shell.execute_reply.started":"2025-05-19T09:38:56.433371Z","shell.execute_reply":"2025-05-19T09:38:56.448011Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"import torch\n\n# Check if PyTorch is using the GPU\nprint(\"Is CUDA available? \", torch.cuda.is_available())\nif device == \"cuda\":\n    print(\"Device name: \", torch.cuda.get_device_name(0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:38:56.461070Z","iopub.execute_input":"2025-05-19T09:38:56.461450Z","iopub.status.idle":"2025-05-19T09:38:56.465705Z","shell.execute_reply.started":"2025-05-19T09:38:56.461434Z","shell.execute_reply":"2025-05-19T09:38:56.464963Z"}},"outputs":[{"name":"stdout","text":"Is CUDA available?  True\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/spamdata-v2/spamdata_v2.csv\")\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:38:56.497798Z","iopub.execute_input":"2025-05-19T09:38:56.498389Z","iopub.status.idle":"2025-05-19T09:38:56.525803Z","shell.execute_reply.started":"2025-05-19T09:38:56.498369Z","shell.execute_reply":"2025-05-19T09:38:56.525213Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"   label                                               text\n0      0  Go until jurong point, crazy.. Available only ...\n1      0                      Ok lar... Joking wif u oni...\n2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n3      0  U dun say so early hor... U c already then say...\n4      0  Nah I don't think he goes to usf, he lives aro...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>Ok lar... Joking wif u oni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>U dun say so early hor... U c already then say...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":27},{"cell_type":"markdown","source":"#### Splitting Dataset","metadata":{}},{"cell_type":"code","source":"# Keeping 70% of data for training and 30% for testing + validation\n### stratify will ensure that the splitted data has same proportion of spam and ham\ntrain_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'], \n                                                                    random_state=2018, \n                                                                    test_size=0.3, \n                                                                    stratify=df['label'])\n\n\n# Now, using the 30% data obtained from previous line to split in \n# 50% test and 50% validation\nval_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n                                                                random_state=2018, \n                                                                test_size=0.5, stratify=temp_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:38:56.527005Z","iopub.execute_input":"2025-05-19T09:38:56.527307Z","iopub.status.idle":"2025-05-19T09:38:56.537563Z","shell.execute_reply.started":"2025-05-19T09:38:56.527290Z","shell.execute_reply":"2025-05-19T09:38:56.537032Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"#### Basic EDA ???","metadata":{}},{"cell_type":"code","source":"# check class distribution\ndf['label'].value_counts(normalize = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:38:56.538723Z","iopub.execute_input":"2025-05-19T09:38:56.538951Z","iopub.status.idle":"2025-05-19T09:38:56.551829Z","shell.execute_reply.started":"2025-05-19T09:38:56.538935Z","shell.execute_reply":"2025-05-19T09:38:56.551060Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"label\n0    0.865937\n1    0.134063\nName: proportion, dtype: float64"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"# get length of all the messages in the train set\nseq_len = [len(i.split()) for i in train_text]\n\npd.Series(seq_len).hist(bins = 30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:38:56.555783Z","iopub.execute_input":"2025-05-19T09:38:56.556303Z","iopub.status.idle":"2025-05-19T09:38:56.750690Z","shell.execute_reply.started":"2025-05-19T09:38:56.556280Z","shell.execute_reply":"2025-05-19T09:38:56.749894Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"<Axes: >"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":30},{"cell_type":"markdown","source":"#### Import BERT - base and uncased","metadata":{}},{"cell_type":"code","source":"# import BERT-base pretrained model\nbert = AutoModel.from_pretrained('bert-base-uncased')\n\n# Load the BERT tokenizer\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:38:56.752014Z","iopub.execute_input":"2025-05-19T09:38:56.752241Z","iopub.status.idle":"2025-05-19T09:38:57.251237Z","shell.execute_reply.started":"2025-05-19T09:38:56.752224Z","shell.execute_reply":"2025-05-19T09:38:57.250636Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"#### Tokenization","metadata":{}},{"cell_type":"markdown","source":"* BERT uses **WordPiece** tokenization.\n* maximum sequence length of the input = 512. This means you cannot feed more than 512 tokens into BERT at once.\n* In below code, we are keeping maximum tokens generated per sequence to 25, and is padded if short or truncated if exceeds.","metadata":{}},{"cell_type":"code","source":"# tokenize and encode sequences in the training set\ntokens_train = tokenizer.batch_encode_plus(\n    train_text.tolist(),\n    max_length = 25,\n    pad_to_max_length=True,\n    truncation=True\n)\n\n# tokenize and encode sequences in the validation set\ntokens_val = tokenizer.batch_encode_plus(\n    val_text.tolist(),\n    max_length = 25,\n    pad_to_max_length=True,\n    truncation=True\n)\n\n# tokenize and encode sequences in the test set\ntokens_test = tokenizer.batch_encode_plus(\n    test_text.tolist(),\n    max_length = 25,\n    pad_to_max_length=True,\n    truncation=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:38:57.252111Z","iopub.execute_input":"2025-05-19T09:38:57.252306Z","iopub.status.idle":"2025-05-19T09:38:57.508529Z","shell.execute_reply.started":"2025-05-19T09:38:57.252292Z","shell.execute_reply":"2025-05-19T09:38:57.507768Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2700: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"* Output of batch_encode_plus is a dictionary with following keys,\n  * input_ids: list of token IDs\n  * token_type_ids: used in tasks like sentence pair classification\n  * attention_mask: indicates which tokens are real tokens and which are padding tokens(0)\n* It is to be noted that in our case, for example, train_text has 3900 samples. Hence, input_ids will be a list of 3900 elements. Each element in this list will be another list of 25 elements (corresponding to max_length argument provided to batch_encode_plus).\n* **Attention masks** is a binary list that tells BERT which tokens are real and which are just padding.\n  * **And why is this needed?** --> BERT operates on fixed-length input sequences (like 25 or 512 tokens). To make every input the same length, we pad shorter sequences with special [PAD] tokens. But we don’t want the model to pay attention to the padding — it’s meaningless. That’s where the attention mask comes in. ","metadata":{}},{"cell_type":"markdown","source":"#### Convert List to Tensors","metadata":{}},{"cell_type":"code","source":"## convert lists to tensors\n\ntrain_seq = torch.tensor(tokens_train['input_ids'])\ntrain_mask = torch.tensor(tokens_train['attention_mask'])\ntrain_y = torch.tensor(train_labels.tolist())\n\nval_seq = torch.tensor(tokens_val['input_ids'])\nval_mask = torch.tensor(tokens_val['attention_mask'])\nval_y = torch.tensor(val_labels.tolist())\n\ntest_seq = torch.tensor(tokens_test['input_ids'])\ntest_mask = torch.tensor(tokens_test['attention_mask'])\ntest_y = torch.tensor(test_labels.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:38:57.510003Z","iopub.execute_input":"2025-05-19T09:38:57.510229Z","iopub.status.idle":"2025-05-19T09:38:57.563382Z","shell.execute_reply.started":"2025-05-19T09:38:57.510212Z","shell.execute_reply":"2025-05-19T09:38:57.562716Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"#### Data Loader","metadata":{}},{"cell_type":"markdown","source":"* This section prepares the input data for batching and feeding into the model efficiently during training and validation.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n#define a batch size\nbatch_size = 32\n\n# wrap tensors\ntrain_data = TensorDataset(train_seq, train_mask, train_y)\n\n# sampler for sampling the data during training\ntrain_sampler = RandomSampler(train_data)\n\n# dataLoader for train set\ntrain_dataloader = DataLoader(train_data, \n                              sampler=train_sampler, \n                              batch_size=batch_size)\n\n# wrap tensors\nval_data = TensorDataset(val_seq, val_mask, val_y)\n\n# sampler for sampling the data during validation\nval_sampler = SequentialSampler(val_data)\n\n# dataLoader for validation set\nval_dataloader = DataLoader(val_data, \n                            sampler = val_sampler, \n                            batch_size=batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:38:57.564094Z","iopub.execute_input":"2025-05-19T09:38:57.564302Z","iopub.status.idle":"2025-05-19T09:38:57.569701Z","shell.execute_reply.started":"2025-05-19T09:38:57.564286Z","shell.execute_reply":"2025-05-19T09:38:57.568876Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"train_data[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:38:57.570590Z","iopub.execute_input":"2025-05-19T09:38:57.571259Z","iopub.status.idle":"2025-05-19T09:38:57.586971Z","shell.execute_reply.started":"2025-05-19T09:38:57.571233Z","shell.execute_reply":"2025-05-19T09:38:57.586304Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"(tensor([ 101, 2035, 2097, 2272, 4142, 1012, 2488, 6149, 2151, 2204, 2559, 3275,\n         2045, 2993, 1012, 1012,  102,    0,    0,    0,    0,    0,    0,    0,\n            0]),\n tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n         0]),\n tensor(0))"},"metadata":{}}],"execution_count":35},{"cell_type":"markdown","source":"#### Model Architecture","metadata":{}},{"cell_type":"code","source":"# freeze all the parameters\nfor param in bert.parameters():\n    param.requires_grad = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:38:57.587752Z","iopub.execute_input":"2025-05-19T09:38:57.587932Z","iopub.status.idle":"2025-05-19T09:38:57.600033Z","shell.execute_reply.started":"2025-05-19T09:38:57.587910Z","shell.execute_reply":"2025-05-19T09:38:57.599348Z"}},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"Now, we have following tasks in front,\n* We have BERT pre-trained model and we have frozen its parameters. This means the parameters will not be updated during back propagation.\n* We will add few fully connected layers to BERT.\n* Add classifier of spam or ham.","metadata":{}},{"cell_type":"markdown","source":"In the new addition to architecture,\n* ensure that FC1 layer has 768 input size because BERT gives vector of 768 values for each sentence ([CLS] token).","metadata":{}},{"cell_type":"code","source":"# nn.Module is the base class for all neural networks in PyTorch\nclass BERT_Arch(nn.Module):\n    def __init__(self, bert):\n        super(BERT_Arch, self).__init__()\n        \n        self.bert = bert     # this is the frozen BERT\n        \n        # dropout layer - randomly disables 10% of neurons during training\n        self.dropout = nn.Dropout(0.1)\n      \n        # relu activation function\n        self.relu =  nn.ReLU()\n\n        # dense layer 1\n        self.fc1 = nn.Linear(768,512)\n      \n        # dense layer 2 (Output layer)\n        self.fc2 = nn.Linear(512,2)\n\n        #softmax activation function\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    #define the forward pass\n    def forward(self, sent_id, mask):\n        \n        #pass the inputs to the model  \n        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n      \n        x = self.fc1(cls_hs)\n\n        x = self.relu(x)\n\n        x = self.dropout(x)\n\n        # output layer\n        x = self.fc2(x)\n      \n        # apply softmax activation\n        x = self.softmax(x)\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:38:57.600933Z","iopub.execute_input":"2025-05-19T09:38:57.601186Z","iopub.status.idle":"2025-05-19T09:38:57.613808Z","shell.execute_reply.started":"2025-05-19T09:38:57.601164Z","shell.execute_reply":"2025-05-19T09:38:57.613054Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# pass the pre-trained BERT to our define architecture\nmodel = BERT_Arch(bert)\n\n# push the model to GPU\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:38:57.614534Z","iopub.execute_input":"2025-05-19T09:38:57.614738Z","iopub.status.idle":"2025-05-19T09:38:57.832303Z","shell.execute_reply.started":"2025-05-19T09:38:57.614715Z","shell.execute_reply":"2025-05-19T09:38:57.831480Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:38:57.835089Z","iopub.execute_input":"2025-05-19T09:38:57.835308Z","iopub.status.idle":"2025-05-19T09:38:57.841071Z","shell.execute_reply.started":"2025-05-19T09:38:57.835292Z","shell.execute_reply":"2025-05-19T09:38:57.840468Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"BERT_Arch(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (relu): ReLU()\n  (fc1): Linear(in_features=768, out_features=512, bias=True)\n  (fc2): Linear(in_features=512, out_features=2, bias=True)\n  (softmax): LogSoftmax(dim=1)\n)"},"metadata":{}}],"execution_count":39},{"cell_type":"markdown","source":"* Next we will define an **optimizer - AdamW**\n* Its a variant of Adam optimizer, and adds weight decay for better regularization.\n* Optimizer controls how model's weights are updated during training.\n* For this,\n  * We give acccess to model's weights.\n  * and tell the optimizer what learning rate it has to adopt.","metadata":{}},{"cell_type":"code","source":"# optimizer from hugging face transformers\n#from transformers import AdamW\nfrom torch.optim import AdamW\n\n# define the optimizer\noptimizer = AdamW(model.parameters(),lr = 1e-5) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:38:57.841660Z","iopub.execute_input":"2025-05-19T09:38:57.841874Z","iopub.status.idle":"2025-05-19T09:38:57.856264Z","shell.execute_reply.started":"2025-05-19T09:38:57.841859Z","shell.execute_reply":"2025-05-19T09:38:57.855509Z"}},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":"Formula used internaally, $$\\text{Weight}_{\\text{class}}=\\frac{n_{\\text{samples}}}{n_{\\text{class}}*n_{\\text{samples in class}}}$$ ","metadata":{}},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\n\n#compute the class weights\nclass_weights = compute_class_weight(class_weight = 'balanced', \n                                     classes = np.unique(train_labels), \n                                     y = train_labels)\n\nprint(\"Class Weights:\",class_weights)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:38:57.856886Z","iopub.execute_input":"2025-05-19T09:38:57.857097Z","iopub.status.idle":"2025-05-19T09:38:57.873246Z","shell.execute_reply.started":"2025-05-19T09:38:57.857072Z","shell.execute_reply":"2025-05-19T09:38:57.872497Z"}},"outputs":[{"name":"stdout","text":"Class Weights: [0.57743559 3.72848948]\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"# converting list of class weights to a tensor\nweights= torch.tensor(class_weights, dtype=torch.float)\n\n# push to GPU\nweights = weights.to(device)\n\n# define the loss function\ncross_entropy  = nn.NLLLoss(weight=weights) \n\n# number of training epochs\nepochs = 10","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:38:57.874350Z","iopub.execute_input":"2025-05-19T09:38:57.874653Z","iopub.status.idle":"2025-05-19T09:38:57.884993Z","shell.execute_reply.started":"2025-05-19T09:38:57.874626Z","shell.execute_reply":"2025-05-19T09:38:57.884412Z"}},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":"#### Fine Tuning the Model","metadata":{}},{"cell_type":"markdown","source":"| Concept                | Explanation                                                                     |\n|------------------------|--------------------------------|\n| `model.train()`        | Sets model to training mode. Enables layers like Dropout and BatchNorm.     |\n| `zero_grad()`          | Clears old gradients before backpropagation.                                |\n| `loss.backward()`      | Computes gradients via backpropagation.                                     |\n| `optimizer.step()`     | Updates model weights using computed gradients.                             |\n| `clip_grad_norm_`      | Prevents exploding gradients by limiting their norm to a max value (e.g., 1).|\n| `detach().cpu().numpy()`| Detaches tensors from GPU and graph for CPU-based post-processing.          |\n| `cross_entropy(...)`   | Computes weighted loss for classification using class imbalance handling.   |\n","metadata":{}},{"cell_type":"code","source":"# function to train the model\ndef train():\n    \n    model.train()\n    total_loss, total_accuracy = 0, 0\n  \n    # empty list to save model predictions\n    total_preds=[]\n\n    # iterate over batches\n    for step,batch in enumerate(train_dataloader):\n        \n        # progress update after every 50 batches.\n        if step % 50 == 0 and not step == 0:\n            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n        \n        # push the batch to gpu\n        batch = [r.to(device) for r in batch]\n \n        sent_id, mask, labels = batch\n\n        # clear previously calculated gradients \n        model.zero_grad()        \n\n        # get model predictions for the current batch\n        preds = model(sent_id, mask)\n\n        # compute the loss between actual and predicted values\n        loss = cross_entropy(preds, labels)\n\n        # add on to the total loss\n        total_loss = total_loss + loss.item()\n\n        # backward pass to calculate the gradients\n        loss.backward()\n\n        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # update parameters\n        optimizer.step()\n\n        # model predictions are stored on GPU. So, push it to CPU\n        preds=preds.detach().cpu().numpy()\n\n    # append the model predictions\n    total_preds.append(preds)\n\n    # compute the training loss of the epoch\n    avg_loss = total_loss / len(train_dataloader)\n  \n    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n    # reshape the predictions in form of (number of samples, no. of classes)\n    total_preds  = np.concatenate(total_preds, axis=0)\n\n    #returns the loss and predictions\n    return avg_loss, total_preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:38:57.885815Z","iopub.execute_input":"2025-05-19T09:38:57.886261Z","iopub.status.idle":"2025-05-19T09:38:57.899619Z","shell.execute_reply.started":"2025-05-19T09:38:57.886238Z","shell.execute_reply":"2025-05-19T09:38:57.898952Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"# function for evaluating the model\ndef evaluate():\n    \n    print(\"\\nEvaluating...\")\n  \n    # deactivate dropout layers\n    model.eval()\n\n    total_loss, total_accuracy = 0, 0\n    \n    # empty list to save the model predictions\n    total_preds = []\n\n    # iterate over batches\n\n    for step,batch in enumerate(val_dataloader):\n        \n        # Progress update every 50 batches.\n        if step % 50 == 0 and not step == 0:\n            \n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            \n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n\n        # push the batch to gpu\n        batch = [t.to(device) for t in batch]\n\n        sent_id, mask, labels = batch\n\n        # deactivate autograd\n        with torch.no_grad():\n            \n            # model predictions\n            preds = model(sent_id, mask)\n\n            # compute the validation loss between actual and predicted values\n            loss = cross_entropy(preds,labels)\n\n            total_loss = total_loss + loss.item()\n\n            preds = preds.detach().cpu().numpy()\n\n            total_preds.append(preds)\n\n    # compute the validation loss of the epoch\n    avg_loss = total_loss / len(val_dataloader) \n\n    # reshape the predictions in form of (number of samples, no. of classes)\n    total_preds  = np.concatenate(total_preds, axis=0)\n\n    return avg_loss, total_preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:38:57.900482Z","iopub.execute_input":"2025-05-19T09:38:57.900764Z","iopub.status.idle":"2025-05-19T09:38:57.918785Z","shell.execute_reply.started":"2025-05-19T09:38:57.900741Z","shell.execute_reply":"2025-05-19T09:38:57.918035Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"# set initial loss to infinite\nbest_valid_loss = float('inf')\n\n# empty lists to store training and validation loss of each epoch\ntrain_losses=[]\nvalid_losses=[]\n\n#for each epoch\nfor epoch in range(epochs):\n     \n    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n    \n    #train model\n    train_loss, _ = train()\n    \n    #evaluate model\n    valid_loss, _ = evaluate()\n    \n    #save the best model\n\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'saved_weights.pt')\n    \n    # append training and validation loss\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n    \n    print(f'\\nTraining Loss: {train_loss:.3f}')\n    print(f'Validation Loss: {valid_loss:.3f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:38:57.919628Z","iopub.execute_input":"2025-05-19T09:38:57.919903Z","iopub.status.idle":"2025-05-19T09:39:51.220074Z","shell.execute_reply.started":"2025-05-19T09:38:57.919882Z","shell.execute_reply":"2025-05-19T09:39:51.219230Z"}},"outputs":[{"name":"stdout","text":"\n Epoch 1 / 10\n  Batch    50  of    122.\n  Batch   100  of    122.\n\nEvaluating...\n\nTraining Loss: 0.668\nValidation Loss: 0.646\n\n Epoch 2 / 10\n  Batch    50  of    122.\n  Batch   100  of    122.\n\nEvaluating...\n\nTraining Loss: 0.636\nValidation Loss: 0.614\n\n Epoch 3 / 10\n  Batch    50  of    122.\n  Batch   100  of    122.\n\nEvaluating...\n\nTraining Loss: 0.611\nValidation Loss: 0.587\n\n Epoch 4 / 10\n  Batch    50  of    122.\n  Batch   100  of    122.\n\nEvaluating...\n\nTraining Loss: 0.588\nValidation Loss: 0.566\n\n Epoch 5 / 10\n  Batch    50  of    122.\n  Batch   100  of    122.\n\nEvaluating...\n\nTraining Loss: 0.563\nValidation Loss: 0.538\n\n Epoch 6 / 10\n  Batch    50  of    122.\n  Batch   100  of    122.\n\nEvaluating...\n\nTraining Loss: 0.545\nValidation Loss: 0.514\n\n Epoch 7 / 10\n  Batch    50  of    122.\n  Batch   100  of    122.\n\nEvaluating...\n\nTraining Loss: 0.520\nValidation Loss: 0.495\n\n Epoch 8 / 10\n  Batch    50  of    122.\n  Batch   100  of    122.\n\nEvaluating...\n\nTraining Loss: 0.505\nValidation Loss: 0.474\n\n Epoch 9 / 10\n  Batch    50  of    122.\n  Batch   100  of    122.\n\nEvaluating...\n\nTraining Loss: 0.490\nValidation Loss: 0.457\n\n Epoch 10 / 10\n  Batch    50  of    122.\n  Batch   100  of    122.\n\nEvaluating...\n\nTraining Loss: 0.470\nValidation Loss: 0.440\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"#load weights of best model\npath = 'saved_weights.pt'\nmodel.load_state_dict(torch.load(path))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:39:51.221038Z","iopub.execute_input":"2025-05-19T09:39:51.221402Z","iopub.status.idle":"2025-05-19T09:39:51.639389Z","shell.execute_reply.started":"2025-05-19T09:39:51.221375Z","shell.execute_reply":"2025-05-19T09:39:51.638818Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":46},{"cell_type":"markdown","source":"### Making Predictions now","metadata":{}},{"cell_type":"code","source":"# get predictions for test data\nwith torch.no_grad():\n    preds = model(test_seq.to(device), test_mask.to(device))\n    preds = preds.detach().cpu().numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:39:51.640068Z","iopub.execute_input":"2025-05-19T09:39:51.640276Z","iopub.status.idle":"2025-05-19T09:39:52.208434Z","shell.execute_reply.started":"2025-05-19T09:39:51.640259Z","shell.execute_reply":"2025-05-19T09:39:52.207688Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"# model's performance\npreds = np.argmax(preds, axis = 1)\nprint(classification_report(test_y, preds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:39:52.209232Z","iopub.execute_input":"2025-05-19T09:39:52.209455Z","iopub.status.idle":"2025-05-19T09:39:52.221356Z","shell.execute_reply.started":"2025-05-19T09:39:52.209430Z","shell.execute_reply":"2025-05-19T09:39:52.220694Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.97      0.86      0.91       724\n           1       0.49      0.84      0.62       112\n\n    accuracy                           0.86       836\n   macro avg       0.73      0.85      0.77       836\nweighted avg       0.91      0.86      0.87       836\n\n","output_type":"stream"}],"execution_count":48}]}